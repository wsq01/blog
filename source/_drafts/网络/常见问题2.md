

# HTTPS 一定安全可靠吗？
这个问题的场景是这样的：客户端通过浏览器向服务端发起 HTTPS 请求时，被「假基站」转发到了一个「中间人服务器」，于是客户端是和「中间人服务器」完成了 TLS 握手，然后这个「中间人服务器」再与真正的服务端完成 TLS 握手。

{% asset_img 1.png %}

具体过程如下：
* 客户端向服务端发起 HTTPS 建立连接请求时，然后被「假基站」转发到了一个「中间人服务器」，接着中间人向服务端发起 HTTPS 建立连接请求，此时客户端与中间人进行 TLS 握手，中间人与服务端进行 TLS 握手；
* 在客户端与中间人进行 TLS 握手过程中，中间人会发送自己的公钥证书给客户端，客户端验证证书的真伪，然后从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给中间人，中间人使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（A），后续客户端与中间人通信就用这个对称加密密钥来加密数据了。
* 在中间人与服务端进行 TLS 握手过程中，服务端会发送从 CA 机构签发的公钥证书给中间人，从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给服务端，服务端使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（B），后续中间人与服务端通信就用这个对称加密密钥来加密数据了。
* 后续的通信过程中，中间人用对称加密密钥（A）解密客户端的 HTTPS 请求的数据，然后用对称加密密钥（B）加密 HTTPS 请求后，转发给服务端，接着服务端发送 HTTPS 响应数据给中间人，中间人用对称加密密钥（B）解密 HTTPS 响应数据，然后再用对称加密密钥（A）加密后，转发给客户端。
从客户端的角度看，其实并不知道网络中存在中间人服务器这个角色。

那么中间人就可以解开浏览器发起的 HTTPS 请求里的数据，也可以解开服务端响应给浏览器的 HTTPS 响应数据。相当于，中间人能够 “偷看” 浏览器与服务端之间的 HTTPS 请求和响应的数据。

但是要发生这种场景是有前提的，前提是用户点击接受了中间人服务器的证书。

中间人服务器与客户端在 TLS 握手过程中，实际上发送了自己伪造的证书给浏览器，而这个伪造的证书是能被浏览器（客户端）识别出是非法的，于是就会提醒用户该证书存在问题。

{% asset_img 2.png %}

如果用户执意点击「继续浏览此网站」，相当于用户接受了中间人伪造的证书，那么后续整个 HTTPS 通信都能被中间人监听了。

所以，这其实并不能说 HTTPS 不够安全，毕竟浏览器都已经提示证书有问题了，如果用户坚决要访问，那不能怪 HTTPS ，得怪自己手贱。
## 客户端是如何验证证书的？
接下来，详细说一下实际中数字证书签发和验证流程。

如下图图所示，为数字证书签发和验证流程：

{% asset_img 3.png %}

当服务端向 CA 机构申请证书的时候，CA 签发证书的过程，如上图左边部分：
* 首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行`Hash`计算，得到一个`Hash`值；
* 然后 CA 会使用自己的私钥将该`Hash`值加密，生成`Certificate Signature`，也就是 CA 对证书做了签名；
* 最后将`Certificate Signature`添加在文件证书上，形成数字证书；

客户端校验服务端的数字证书的过程，如上图右边部分：
* 首先客户端会使用同样的`Hash`算法获取该证书的`Hash`值 H1；
* 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密`Certificate Signature`内容，得到一个`Hash`值 H2 ；
* 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。

但事实上，证书的验证过程中还存在一个证书信任链的问题，因为我们向 CA 申请的证书一般不是根证书签发的，而是由中间证书签发的，比如百度的证书，从下图你可以看到，证书的层级有三级：

{% asset_img 4.png %}

对于这种三级层级关系的证书的验证过程如下：
* 客户端收到`baidu.com`的证书后，发现这个证书的签发者不是根证书，就无法根据本地已有的根证书中的公钥去验证`baidu.com`证书是否可信。于是，客户端根据`baidu.com`证书中的签发者，找到该证书的颁发机构是`GlobalSign Organization Validation CA - SHA256 - G2`，然后向 CA 请求该中间证书。
* 请求到证书后发现`GlobalSign Organization Validation CA - SHA256 - G2`证书是由`GlobalSign Root CA`签发的，由于`GlobalSign Root CA`没有再上级签发机构，说明它是根证书，也就是自签证书。应用软件会检查此证书有否已预载于根证书清单上，如果有，则可以利用根证书中的公钥去验证`GlobalSign Organization Validation CA - SHA256 - G2`证书，如果发现验证通过，就认为该中间证书是可信的。
* `GlobalSign Organization Validation CA - SHA256 - G2`证书被信任后，可以使用`GlobalSign Organization Validation CA - SHA256 - G2`证书中的公钥去验证 baidu.com 证书的可信性，如果验证通过，就可以信任`baidu.com`证书。

在这四个步骤中，最开始客户端只信任根证书`GlobalSign Root CA`证书的，然后`GlobalSign Root CA`证书信任`GlobalSign Organization Validation CA - SHA256 - G2`证书，而`GlobalSign Organization Validation CA - SHA256 - G2`证书又信任`baidu.com`证书，于是客户端也信任`baidu.com`证书。总括来说，由于用户信任`GlobalSign`，所以由`GlobalSign`所担保的`baidu.com`可以被信任，另外由于用户信任操作系统或浏览器的软件商，所以由软件商预载了根证书的`GlobalSign`都可被信任。

{% asset_img 5.png %}

操作系统里一般都会内置一些根证书：

{% asset_img 6.png %}

这样的一层层地验证就构成了一条信任链路，整个证书信任链验证流程如下图所示：

{% asset_img 7.png %}

如果你的电脑中毒了，被恶意导入了中间人的根证书，那么在验证中间人的证书的时候，由于你操作系统信任了中间人的根证书，那么等同于中间人的证书是合法的。

这种情况下，浏览器是不会弹出证书存在问题的风险提醒的。

这其实也不关 HTTPS 的事情，是你电脑中毒了才导致 HTTPS 数据被中间人劫持的。

所以，HTTPS 协议本身到目前为止还是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全。
## 为什么抓包工具能截取 HTTPS 数据？
抓包工具 Fiddler 之所以可以明文看到 HTTPS 数据，工作原理与中间人一致的。

对于 HTTPS 连接来说，中间人要满足以下两点，才能实现真正的明文代理:

中间人，作为客户端与真实服务端建立连接这一步不会有问题，因为服务端不会校验客户端的身份；
中间人，作为服务端与真实客户端建立连接，这里会有客户端信任服务端的问题，也就是服务端必须有对应域名的私钥；

中间人要拿到私钥只能通过如下方式：
* 去网站服务端拿到私钥；
* 去CA处拿域名签发私钥；
* 自己签发证书，且被浏览器信任；
  
不用解释，抓包工具只能使用第三种方式取得中间人的身份。

使用抓包工具进行 HTTPS 抓包的时候，需要在客户端安装 Fiddler 的根证书，这里实际上起认证中心（CA）的作用。

Fiddler 能够抓包的关键是客户端会往系统受信任的根证书列表中导入 Fiddler 生成的证书，而这个证书会被浏览器信任，也就是 Fiddler 给自己创建了一个认证中心 CA。

客户端拿着中间人签发的证书去中间人自己的 CA 去认证，当然认为这个证书是有效的。
## 如何避免被中间人抓取数据？
我们要保证自己电脑的安全，不要被病毒乘虚而入，而且也不要点击任何证书非法的网站，这样 HTTPS 数据就不会被中间人截取到了。

当然，我们还可以通过 HTTPS 双向认证来避免这种问题。

一般我们的 HTTPS 是单向认证，客户端只会验证了服务端的身份，但是服务端并不会验证客户端的身份。

如果用了双向认证方式，不仅客户端会验证服务端的身份，而且服务端也会验证客户端的身份。

{% asset_img 8.png %}

服务端一旦验证到请求自己的客户端为不可信任的，服务端就拒绝继续通信，客户端如果发现服务端为不可信任的，那么也中止通信。

# HTTPS 会加密 URL 吗？
答案是，会加密的。

因为 URL 的信息都是保存在 HTTP Header 中的，而 HTTPS 是会对 HTTP Header + HTTP Body 整个加密的，所以 URL 自然是会被加密的。

下图是 HTTP/1.1 的请求头部，可以看到是包含 URL 信息的。

{% asset_img 11.png %}

对应的实际的 HTTP/1.1 的请求头部：

{% asset_img 12.png %}

HTTP/1.1 请求的第一行包含请求方法和路径。HTTP/2 用一系列伪头部替换了请求行，这五个伪头部很容易识别，因为它们在名称的开头用了一个冒号来表示。

比如请求方法和路径伪头字段如下：
* ":method" 伪头字段包含了 HTTP 方法；
* ":path" 伪头字段包含目标 URL 的路径和查询部分；

{% asset_img 13.png %}

上图是我浏览器 F12 开发者工具查看的信息，浏览器显示信息是已经解密后的信息，所以不要误以为 URL 没有加密。

如果你用抓包工具，抓包 HTTPS 的数据的话，你是什么都看不到的，如下图，只会显示`Application Data`，表示这是一个已经加密的 HTTP 应用数据。

{% asset_img 14.png %}

## HTTPS 可以看到域名吗？
从上面我们知道，HTTPS 是已经把 HTTP Header + HTTP Body 整个加密的，所以我们是无法从加密的 HTTP 数据中获取请求的域名的。

但是我们可以在 TLS 握手过程中看到域名信息。

比如下图，TLS 第一次握手的 “Client Hello” 消息中，有个`server name`字段，它就是请求的域名地址。

{% asset_img 15.png %}

所以，用了 HTTPS 也不能以为偷偷在公司上某 hub 不会被发现。
## HTTPS 的应用数据是如何保证完整性的？
然后很多读者以为 HTTP 数据就用对称加密密钥（TLS 握手过程中协商出来的对称加密密钥）加密后就直接发送了，然后就疑惑 HTTP 数据有没有通过摘要算法来保证完整性？

事实上，TLS 在实现上分为握手协议和记录协议两层：
* TLS 握手协议就是我们说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）；
* TLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议；

TLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，过程如下图：

{% asset_img 16.png %}

具体过程如下：
* 首先，消息被分割成多个较短的片段,然后分别对每个片段进行压缩。
* 接下来，经过压缩的片段会被加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。
* 再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。
* 最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的加密报文数据。

记录协议完成后，最终的加密报文数据将传递到传输控制协议 (TCP) 层进行传输。

# TCP 和 UDP 可以同时绑定相同的端口吗？
答案：可以的。

TCP 和 UDP 服务端网络相似的一个地方，就是会调用`bind`绑定端口。

TCP 网络编程如下，服务端执行`listen()`系统调用就是监听端口的动作。

{% asset_img 21.png TCP网络编程 %}

UDP 网络编程如下，服务端是没有监听这个动作的，只有执行`bind()`系统调用来绑定端口的动作。

{% asset_img 22.png UDP网络编程 %}

在数据链路层中，通过 MAC 地址来寻找局域网中的主机。在网际层中，通过 IP 地址来寻找网络中互连的主机或路由器。在传输层中，需要通过端口进行寻址，来识别同一计算机中同时通信的不同应用程序。

所以，传输层的「端口号」的作用，是为了区分同一个主机上不同应用程序的数据包。

传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。

当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。

{% asset_img 23.png %}

因此， TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。
# 多个 TCP 服务进程可以绑定同一个端口吗？
还是以前面的 TCP 服务端程序作为例子，启动两个同时绑定同一个端口的 TCP 服务进程。

运行第一个 TCP 服务进程之后，`netstat`命令可以查看，8888 端口已经被一个 TCP 服务进程绑定并监听了。

接着，运行第二个 TCP 服务进程的时候，就报错了`Address already in use`。

上面的情况是两个 TCP 服务进程同时绑定地址和端口是：`0.0.0.0`地址和 8888 端口，所以才出现的错误。

如果两个 TCP 服务进程绑定的 IP 地址不同，而端口相同的话，也是可以绑定成功的，如下图：

{% asset_img 27.png %}

所以，默认情况下，针对「多个 TCP 服务进程可以绑定同一个端口吗？」这个问题的答案是：如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行`bind()`时候就会出错，错误是`Address already in use`。

注意，如果 TCP 服务进程 A 绑定的地址是`0.0.0.0`和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。

这是因为`0.0.0.0`地址比较特殊，代表任意地址，意味着绑定了`0.0.0.0`地址，相当于把主机上的所有 IP 地址都绑定了。
## 重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？
TCP 服务进程需要绑定一个 IP 地址和一个端口，然后就监听在这个地址和端口上，等待客户端连接的到来。

然后在实践中，我们可能会经常碰到一个问题，当 TCP 服务进程重启之后，总是碰到`Address in use`的报错信息，TCP 服务进程不能很快地重启，而是要过一会才能重启成功。

当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在`TIME_WAIT`这个状态里停留一段时间，这个时间大约为 2MSL。

{% asset_img 24.png %}

当 TCP 服务进程重启时，服务端会出现`TIME_WAIT`状态的连接，`TIME_WAIT`状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行`bind()`函数的时候，就会返回了`Address already in use`的错误。

而等`TIME_WAIT`状态的连接结束后，重启 TCP 服务进程就能成功。
## 重启 TCP 服务进程时，如何避免“Address in use”的报错信息？
我们可以在调用`bind`前，对`socket`设置`SO_REUSEADDR`属性，可以解决这个问题。
```
int on = 1;
setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
```
因为`SO_REUSEADDR`作用是：如果当前启动进程绑定的 IP+PORT 与处于`TIME_WAIT`状态的连接占用的 IP+PORT 存在冲突，但是新启动的进程使用了`SO_REUSEADDR`选项，那么该进程就可以绑定成功。

举个例子，服务端有个监听`0.0.0.0`地址和 8888 端口的 TCP 服务进程。‍

有个客户端（IP地址：`192.168.1.100`）已经和服务端（IP 地址：`172.19.11.200`）建立了 TCP 连接，那么在 TCP 服务进程重启时，服务端会与客户端经历四次挥手，服务端的 TCP 连接会短暂处于`TIME_WAIT`状态：
```
客户端地址:端口           服务端地址:端口        TCP 连接状态
192.168.1.100:37272     172.19.11.200:8888    TIME_WAIT
```
如果 TCP 服务进程没有对`socket`设置`SO_REUSEADDR`属性，那么在重启时，由于存在一个和绑定 IP+PORT 一样的`TIME_WAIT`状态的连接，那么在执行`bind()`函数的时候，就会返回了`Address already in use`的错误。

如果 TCP 服务进程对`socket`设置`SO_REUSEADDR`属性了，那么在重启时，即使存在一个和绑定 IP+PORT 一样的`TIME_WAIT`状态的连接，依然可以正常绑定成功，因此可以正常重启成功。

因此，在所有 TCP 服务器程序中，调用`bind`之前最好对`socket`设置`SO_REUSEADDR`属性，这不会产生危害，相反，它会帮助我们在很快时间内重启服务端程序。‍

前面我提到过这个问题：如果 TCP 服务进程 A 绑定的地址是`0.0.0.0`和端口 8888，而如果 TCP 服务进程 B 绑定的地址是`192.168.1.100`地址（或者其他地址）和端口 8888，那么执行`bind()`时候也会出错。

这个问题也可以由`SO_REUSEADDR`解决，因为它的另外一个作用是：绑定的 IP地址 + 端口时，只要 IP 地址不是正好相同，那么允许绑定。

比如，`0.0.0.0:8888`和`192.168.1.100:8888`，虽然逻辑意义上前者包含了后者，但是`0.0.0.0`泛指所有本地 IP，而`192.168.1.100`特指某一 IP，两者并不是完全相同，所以在对`socket`设置`SO_REUSEADDR`属性后，那么执行`bind()`时候就会绑定成功。
## 客户端的端口可以重复使用吗？
客户端在执行`connect`函数的时候，会在内核里随机选择一个端口，然后向服务端发起 SYN 报文，然后与服务端进行三次握手。

{% asset_img 25.png %}

所以，客户端的端口选择的发生在`connect`函数，内核在选择端口的时候，会从`net.ipv4.ip_local_port_range`这个内核参数指定的范围来选取一个端口作为客户端端口。

该参数的默认值是`32768 61000`，意味着端口总可用的数量是`61000 - 32768 = 28232`个。

当客户端与服务端完成 TCP 连接建立后，我们可以通过`netstat`命令查看 TCP 连接。
```
$ netstat -napt
```
协议  源ip地址:端口            目的ip地址：端口         状态
tcp  192.168.110.182.64992   117.147.199.51.443     ESTABLISHED

那问题来了，上面客户端已经用了 64992 端口，那么还可以继续使用该端口发起连接吗？

这个问题，很多同学都会说不可以继续使用该端口了，如果按这个理解的话， 默认情况下客户端可以选择的端口是 28232 个，那么意味着客户端只能最多建立  28232 个 TCP 连接，如果真是这样的话，那么这个客户端并发连接也太少了吧，所以这是错误理解。

正确的理解是，TCP 连接是由四元组（源 IP 地址，源端口，目的 IP 地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。所以如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。

比如下面这张图，有 2 个 TCP 连接，左边是客户端，右边是服务端，客户端使用了相同的端口 50004 与两个服务端建立了 TCP 连接。

{% asset_img 28.jpg %}

仔细看，上面这两条 TCP 连接的四元组信息中的「目的 IP 地址」是不同的，一个是`180.101.49.12`，另外一个是`180.101.49.11`。
## 多个客户端可以`bind`同一个端口吗？
`bind`函数虽然常用于服务端网络编程中，但是它也是用于客户端的。

前面我们知道，客户端是在调用`connect`函数的时候，由内核随机选取一个端口作为连接的端口。

而如果我们想自己指定连接的端口，就可以用`bind`函数来实现：客户端先通过`bind`函数绑定一个端口，然后调用`connect`函数就会跳过端口选择的过程了，转而使用`bind`时确定的端口。

针对这个问题：多个客户端可以`bind`同一个端口吗？

要看多个客户端绑定的 IP + PORT 是否都相同，如果都是相同的，那么在执行`bind()`时候就会出错，错误是`Address already in use`。

如果一个绑定在`192.168.1.100:6666`，一个绑定在`192.168.1.200:6666`，因为 IP 不相同，所以执行`bind()`的时候，能正常绑定。

所以， 如果多个客户端同时绑定的 IP 地址和端口都是相同的，那么执行`bind()`时候就会出错，错误是`Address already in use`。

一般而言，客户端不建议使用`bind`函数，应该交由`connect`函数来选择端口会比较好，因为客户端的端口通常都没什么意义。

客户端 TCP 连接`TIME_WAIT`状态过多，会导致端口资源耗尽而无法建立新的连接吗？

针对这个问题要看，客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。

如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端`TIME_WAIT`状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。

但是，因为只要客户端连接的服务器不同，端口资源可以重复使用的。

所以，如果客户端都是与不同的服务器建立连接，即使客户端端口资源只有几万个， 客户端发起百万级连接也是没问题的（当然这个过程还会受限于其他资源，比如文件描述符、内存、CPU 等）。

如何解决客户端 TCP 连接`TIME_WAIT`过多，导致无法与同一个服务器建立连接的问题？

前面我们提到，如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端`TIME_WAIT`状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。

针对这个问题，也是有解决办法的，那就是打开`net.ipv4.tcp_tw_reuse`这个内核参数。

因为开启了这个内核参数后，客户端调用`connect`函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于`TIME_WAIT`状态，如果该连接处于`TIME_WAIT`状态并且`TIME_WAIT`状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。

举个例子，假设客户端已经与服务器建立了一个 TCP 连接，并且这个状态处于`TIME_WAIT`状态：
```
客户端地址:端口           服务端地址:端口         TCP 连接状态
192.168.1.100:2222      172.19.11.21:8888     TIME_WAIT
```
然后客户端又与该服务器（`172.19.11.21:8888`）发起了连接，在调用`connect`函数时，内核刚好选择了 2222 端口，接着发现已经被相同四元组的连接占用了：
* 如果没有开启`net.ipv4.tcp_tw_reuse`内核参数，那么内核就会选择下一个端口，然后继续判断，直到找到一个没有被相同四元组的连接使用的端口， 如果端口资源耗尽还是没找到，那么`connect`函数就会返回错误。
* 如果开启了`net.ipv4.tcp_tw_reuse`内核参数，就会判断该四元组的连接状态是否处于`TIME_WAIT`状态，如果连接处于`TIME_WAIT`状态并且该状态持续的时间超过了 1 秒，那么就会重用该连接，于是就可以使用 2222 端口了，这时`connect`就会返回成功。

再次提醒一次，开启了`net.ipv4.tcp_tw_reuse`内核参数，是客户端（连接发起方） 在调用`connect()`函数时才起作用，所以在服务端开启这个参数是没有效果的。
## 客户端端口选择的流程总结
至此，我们已经把客户端在执行`connect`函数时，内核选择端口的情况大致说了一遍，为了让大家更明白客户端端口的选择过程，我画了一流程图。

{% asset_img 26.jpg %}

## 总结
TCP 和 UDP 可以同时绑定相同的端口吗？可以的。

TCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。

当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。

因此， TCP/UDP 各自的端口号也相互独立，互不影响。

多个 TCP 服务进程可以同时绑定同一个端口吗？

如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行`bind()`时候就会出错，错误是`Address already in use`。

如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行`bind()`不会出错。

如何解决服务端重启时，报错`Address already in use`的问题？

当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在`TIME_WAIT`这个状态里停留一段时间，这个时间大约为 2MSL。

当 TCP 服务进程重启时，服务端会出现`TIME_WAIT`状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行`bind()`函数的时候，就会返回了`Address already in use`的错误。

要解决这个问题，我们可以对`socket`设置`SO_REUSEADDR`属性。

这样即使存在一个和绑定 IP+PORT 一样的`TIME_WAIT`状态的连接，依然可以正常绑定成功，因此可以正常重启成功。

客户端的端口可以重复使用吗？

在客户端执行`connect`函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。

TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。

所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。

客户端 TCP 连接`TIME_WAIT`状态过多，会导致端口资源耗尽而无法建立新的连接吗？

要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。

如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端`TIME_WAIT`状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。

如何解决客户端 TCP 连接`TIME_WAIT`过多，导致无法与同一个服务器建立连接的问题？

打开`net.ipv4.tcp_tw_reuse`这个内核参数。

因为开启了这个内核参数后，客户端调用`connect`函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于`TIME_WAIT`状态。

如果该连接处于`TIME_WAIT`状态并且`TIME_WAIT`状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。
# 服务端大量处于 TIME_WAIT 状态连接的原因。
我们先来看一下 TCP 四次挥手的流程吧，看看`TIME_WAIT`状态发生在哪一个阶段。

下面这个图，是由「客户端」作为「主动关闭方」的 TCP 四次挥手的流程。

{% asset_img 31.png TCP四次挥手的流程 %}

从上面我们可以知道，`TIME_WAIT`状态是「主动关闭连接方」才会出现的状态。而且`TIME_WAIT`状态会持续 2MSL 时间才会进入到`close`状态。在 Linux 上 2MSL 的时长是 60 秒，也就是说停留在`TIME_WAIT`的时间为固定的 60 秒。

为什么需要`TIME_WAIT`状态？主要有两个原因：
1. 保证「被动关闭连接」的一方，能被正确的关闭。TCP 协议在关闭连接的四次挥手中，在主动关闭方发送的最后一个 ACK 报文，有可能丢失，这时被动方会重新发 FIN 报文, 如果这时主动方处于 CLOSE 状态 ，就会响应 RST 报文而不是 ACK 报文。所以主动方要处于`TIME_WAIT`状态，而不能是 CLOSE。
2. 防止历史连接中的数据，被后面相同四元组的连接错误的接收。TCP 报文可能由于路由器异常而 “迷路”，在迷途期间，TCP 发送端可能因确认超时而重发这个报文，迷途的报文在路由器修复后也会被送到最终目的地，这个原来的迷途报文就称为`lost duplicate`。在关闭一个 TCP 连接后，马上又重新建立起一个相同的 IP 地址和端口之间的 TCP 连接，后一个连接被称为前一个连接的化身，那么有可能出现这种情况，前一个连接的迷途重复报文在前一个连接终止后出现，从而被误解成从属于新的化身。为了避免这个情况，`TIME_WAIT`状态需要持续 2MSL，因为这样就可以保证当成功建立一个 TCP 连接的时候，来自连接先前化身的重复报文已经在网络中消逝。

很多人误解以为只有客户端才会有`TIME_WAIT`状态，这是不对的。TCP 是全双工协议，哪一方都可以先关闭连接，所以哪一方都可能会有`TIME_WAIT`状态。

总之记住，谁先关闭连接的，它就是主动关闭方，那么`TIME_WAIT`就会出现在主动关闭方。
## 什么场景下服务端会主动断开连接呢？
如果服务端出现大量的`TIME_WAIT`状态的 TCP 连接，就是说明服务端主动断开了很多 TCP 连接。

问题来了，什么场景下服务端会主动断开连接呢？
* 第一个场景：HTTP 没有使用长连接
* 第二个场景：HTTP 长连接超时
* 第三个场景：HTTP 长连接的请求数量达到上限

## 第一个场景：HTTP 没有使用长连接
我们先来看看 HTTP 长连接（Keep-Alive）机制是怎么开启的。

在 HTTP/1.0 中默认是关闭的，如果浏览器要开启`Keep-Alive`，它必须在请求的`header`中添加：
```
Connection: Keep-Alive
```
然后当服务器收到请求，作出回应的时候，它也被添加到响应中`header`里：
```
Connection: Keep-Alive
```
这样做，TCP 连接就不会中断，而是保持连接。当客户端发送另一个请求时，它会使用同一个 TCP 连接。这一直继续到客户端或服务器端提出断开连接。

从 HTTP/1.1 开始， 就默认是开启了 Keep-Alive，现在大多数浏览器都默认是使用 HTTP/1.1，所以 Keep-Alive 都是默认打开的。一旦客户端和服务端达成协议，那么长连接就建立好了。

如果要关闭 HTTP Keep-Alive，需要在 HTTP 请求或者响应的`header`里添加`Connection:close`信息，也就是说，只要客户端和服务端任意一方的 HTTP header 中有`Connection:close`信息，那么就无法使用 HTTP 长连接的机制。

关闭 HTTP 长连接机制后，每次请求都要经历这样的过程：建立 TCP -> 请求资源 -> 响应资源 -> 释放连接，那么此方式就是 HTTP 短连接，如下图：

{% asset_img 32.png %}

在前面我们知道，只要任意一方的 HTTP header 中有`Connection:close`信息，就无法使用 HTTP 长连接机制，这样在完成一次 HTTP 请求/处理后，就会关闭连接。

问题来了，这时候是客户端还是服务端主动关闭连接呢？

在 RFC 文档中，并没有明确由谁来关闭连接，请求和响应的双方都可以主动关闭 TCP 连接。

不过，根据大多数 Web 服务的实现，不管哪一方禁用了 HTTP Keep-Alive，都是由服务端主动关闭连接，那么此时服务端上就会出现`TIME_WAIT`状态的连接。

客户端禁用了 HTTP Keep-Alive，服务端开启 HTTP Keep-Alive，谁是主动关闭方？

当客户端禁用了 HTTP Keep-Alive，这时候 HTTP 请求的 header 就会有`Connection:close`信息，这时服务端在发完 HTTP 响应后，就会主动关闭连接。

为什么要这么设计呢？HTTP 是请求-响应模型，发起方一直是客户端，HTTP Keep-Alive 的初衷是为客户端后续的请求重用连接，如果我们在某次 HTTP 请求-响应模型中，请求的`header`定义了`connection：close`信息，那不再重用这个连接的时机就只有在服务端了，所以我们在 HTTP 请求-响应这个周期的「末端」关闭连接是合理的。

客户端开启了 HTTP Keep-Alive，服务端禁用了 HTTP Keep-Alive，谁是主动关闭方？

当客户端开启了 HTTP Keep-Alive，而服务端禁用了 HTTP Keep-Alive，这时服务端在发完 HTTP 响应后，服务端也会主动关闭连接。

为什么要这么设计呢？在服务端主动关闭连接的情况下，只要调用一次`close()`就可以释放连接，剩下的工作由内核 TCP 栈直接进行了处理，整个过程只有一次`syscall`；如果是要求客户端关闭，则服务端在写完最后一个`response`之后需要把这个`socket`放入`readable`队列，调用`select / epoll`去等待事件；然后调用一次`read()`才能知道连接已经被关闭，这其中是两次`syscall`，多一次用户态程序被激活执行，而且`socket`保持时间也会更长。

因此，当服务端出现大量的`TIME_WAIT`状态连接的时候，可以排查下是否客户端和服务端都开启了 HTTP Keep-Alive，因为任意一方没有开启 HTTP Keep-Alive，都会导致服务端在处理完一个 HTTP 请求后，就主动关闭连接，此时服务端上就会出现大量的`TIME_WAIT`状态的连接。

针对这个场景下，解决的方式也很简单，让客户端和服务端都开启 HTTP Keep-Alive 机制。
## 第二个场景：HTTP 长连接超时
HTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。

HTTP 长连接可以在同一个 TCP 连接上接收和发送多个 HTTP 请求/应答，避免了连接建立和释放的开销。

{% asset_img 33.png %}

如果使用了 HTTP 长连接，如果客户端完成一个 HTTP 请求后，就不再发起新的请求，此时这个 TCP 连接一直占用着不是挺浪费资源的吗？

对没错，所以为了避免资源浪费的情况，web 服务软件一般都会提供一个参数，用来指定 HTTP 长连接的超时时间，比如 nginx 提供的`keepalive_timeout`参数。

假设设置了 HTTP 长连接的超时时间是 60 秒，nginx 就会启动一个「定时器」，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，定时器的时间一到，nginx 就会触发回调函数来关闭该连接，那么此时服务端上就会出现`TIME_WAIT`状态的连接。

{% asset_img 34.png %}

当服务端出现大量`TIME_WAIT`状态的连接时，如果现象是有大量的客户端建立完 TCP 连接后，很长一段时间没有发送数据，那么大概率就是因为 HTTP 长连接超时，导致服务端主动关闭连接，产生大量处于`TIME_WAIT`状态的连接。

可以往网络问题的方向排查，比如是否是因为网络问题，导致客户端发送的数据一直没有被服务端接收到，以至于 HTTP 长连接超时。
## 第三个场景：HTTP 长连接的请求数量达到上限
Web 服务端通常会有个参数，来定义一条 HTTP 长连接上最大能处理的请求数量，当超过最大限制时，就会主动关闭连接。

比如`nginx`的`keepalive_requests`这个参数，这个参数是指一个 HTTP 长连接建立之后，nginx 就会为这个连接设置一个计数器，记录这个 HTTP 长连接上已经接收并处理的客户端请求的数量。如果达到这个参数设置的最大值时，则 nginx 会主动关闭这个长连接，那么此时服务端上就会出现 TIME_WAIT 状态的连接。

`keepalive_requests`参数的默认值是 100 ，意味着每个 HTTP 长连接最多只能跑 100  次请求，这个参数往往被大多数人忽略，因为当 QPS (每秒请求数) 不是很高时，默认值 100 凑合够用。

但是，对于一些 QPS 比较高的场景，比如超过 10000 QPS，甚至达到 30000 , 50000 甚至更高，如果 keepalive_requests 参数值是 100，这时候就 nginx 就会很频繁地关闭连接，那么此时服务端上就会出大量的`TIME_WAIT`状态。

针对这个场景下，解决的方式也很简单，调大 nginx 的`keepalive_requests`参数就行。

## TIME_WAIT 状态过多有什么危害？
过多的`TIME-WAIT`状态主要的危害有两种：
* 第一是占用系统资源，比如文件描述符、内存资源、CPU 资源等；
* 第二是占用端口资源，端口资源也是有限的，一般可以开启的端口为 32768～61000，也可以通过`net.ipv4.ip_local_port_range`参数指定范围。

客户端和服务端`TIME_WAIT`过多，造成的影响是不同的。

如果客户端（主动发起关闭连接方）的 TIME_WAIT 状态过多，占满了所有端口资源，那么就无法对「目的 IP+ 目的 PORT」都一样的服务端发起连接了，但是被使用的端口，还是可以继续对另外一个服务端发起连接的。

因此，客户端（发起连接方）都是和「目的 IP+ 目的 PORT 」都一样的服务端建立连接的话，当客户端的 TIME_WAIT 状态连接过多的话，就会受端口资源限制，如果占满了所有端口资源，那么就无法再跟「目的 IP+ 目的 PORT」都一样的服务端建立连接了。

不过，即使是在这种场景下，只要连接的是不同的服务端，端口是可以重复使用的，所以客户端还是可以向其他服务端发起连接的，这是因为内核在定位一个连接的时候，是通过四元组（源IP、源端口、目的IP、目的端口）信息来定位的，并不会因为客户端的端口一样，而导致连接冲突。

如果服务端（主动发起关闭连接方）的 TIME_WAIT 状态过多，并不会导致端口资源受限，因为服务端只监听一个端口，而且由于一个四元组唯一确定一个 TCP 连接，因此理论上服务端可以建立很多连接，但是 TCP 连接过多，会占用系统资源，比如文件描述符、内存资源、CPU 资源等。

## 如何优化 TIME_WAIT 状态？
这里给出优化`TIME-WAIT`的几个方式，都是有利有弊：

打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；
```
net.ipv4.tcp_max_tw_buckets
```
程序中使用 SO_LINGER ，应用强制使用 RST 关闭。
方式一：net.ipv4.tcp_tw_reuse 和 tcp_timestamps

开启 tcp_tw_reuse，则可以复用处于 TIME_WAIT 的 socket 为新的连接所用。

有一点需要注意的是，tcp_tw_reuse 功能只能用客户端（连接发起方），因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用。
```
net.ipv4.tcp_tw_reuse = 1
```
使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即
```
net.ipv4.tcp_timestamps=1（默认即为 1）
```
这个时间戳的字段是在 TCP 头部的「选项」里，它由一共 8 个字节表示时间戳，其中第一个 4 字节字段用来保存发送该数据包的时间，第二个 4 字节字段用来保存最近一次接收对方发送到达数据的时间。

由于引入了时间戳，可以使得重复的数据包会因为时间戳过期被自然丢弃，因此 TIME_WAIT 状态才可以被复用。

方式二：`net.ipv4.tcp_max_tw_buckets`

这个值默认为 18000，当系统中处于`TIME_WAIT`的连接一旦超过这个值时，系统就会将后面的`TIME_WAIT`连接状态重置，这个方法比较暴力。
```
net.ipv4.tcp_max_tw_buckets = 18000
```
方式三：程序中使用 SO_LINGER

我们可以通过设置 socket 选项，来设置调用 close 关闭连接行为。
```
struct linger so_linger;
so_linger.l_onoff = 1;
so_linger.l_linger = 0;
setsockopt(s, SOL_SOCKET, SO_LINGER, &so_linger,sizeof(so_linger));
```
如果`l_onoff`为非 0， 且`l_linger`值为 0，那么调用`close`后，会立该发送一个RST标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了`TIME_WAIT`状态，直接关闭。

但这为跨越`TIME_WAIT`状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。

前面介绍的方法都是试图越过`TIME_WAIT`状态的，这样其实不太好。虽然 TIME_WAIT 状态持续的时间是有一点长，显得很不友好，但是它被设计来就是用来避免发生乱七八糟的事情。

TIME_WAIT 是我们的朋友，它是有助于我们的，不要试图避免这个状态，而是应该弄清楚它。

如果服务端要避免过多的 TIME_WAIT 状态的连接，就永远不要主动断开连接，让客户端去断开，由分布在各处的客户端去承受 TIME_WAIT。
# 服务端大量处于 CLOSE_WAIT 状态连接的原因

{% asset_img 31.png %}

从上面这张图我们可以得知，`CLOSE_WAIT`状态是「被动关闭方」才会有的状态，而且如果「被动关闭方」没有调用`close`函数关闭连接，那么就无法发出`FIN`报文，从而无法使得`CLOSE_WAIT`状态的连接转变为`LAST_ACK`状态。

所以，当服务端出现大量`CLOSE_WAIT`状态的连接的时候，说明服务端的程序没有调用`close`函数关闭连接。

那什么情况会导致服务端的程序没有调用`close`函数关闭连接？这时候通常需要排查代码。

我们先来分析一个普通的 TCP 服务端的流程：
1. 创建服务端`socket，bind`绑定端口、`listen`监听端口
2. 将服务端`socket`注册到`epoll`
3. `epoll_wait`等待连接到来，连接到来时，调用`accpet`获取已连接的`socket`
4. 将已连接的`socket`注册到`epoll`
5. `epoll_wait`等待事件发生
6. 对方连接关闭时，我方调用`close`

可能导致服务端没有调用`close`函数的原因，如下。

第一个原因：第 2 步没有做，没有将服务端`socket`注册到`epoll`，这样有新连接到来时，服务端没办法感知这个事件，也就无法获取到已连接的`socket`，那服务端自然就没机会对`socket`调用 close 函数了。

不过这种原因发生的概率比较小，这种属于明显的代码逻辑 bug，在前期 read view 阶段就能发现的了。

第二个原因：第 3 步没有做，有新连接到来时没有调用`accpet`获取该连接的`socket`，导致当有大量的客户端主动断开了连接，而服务端没机会对这些`socket`调用`close`函数，从而导致服务端出现大量`CLOSE_WAIT`状态的连接。

发生这种情况可能是因为服务端在执行`accpet`函数之前，代码卡在某一个逻辑或者提前抛出了异常。

第三个原因：第 4 步没有做，通过`accpet`获取已连接的`socket`后，没有将其注册到`epoll`，导致后续收到 FIN 报文的时候，服务端没办法感知这个事件，那服务端就没机会调用`close`函数了。

发生这种情况可能是因为服务端在将已连接的`socket`注册到`epoll`之前，代码卡在某一个逻辑或者提前抛出了异常。之前看到过别人解决`close_wait`问题的实践文章，感兴趣的可以看看：一次 Netty 代码不健壮导致的大量`CLOSE_WAIT`连接原因分析

第四个原因：第 6 步没有做，当发现客户端关闭连接后，服务端没有执行`close`函数，可能是因为代码漏处理，或者是在执行`close`函数之前，代码卡在某一个逻辑，比如发生死锁等等。

可以发现，当服务端出现大量`CLOSE_WAIT`状态的连接的时候，通常都是代码的问题，这时候我们需要针对具体的代码一步一步的进行排查和定位，主要分析的方向就是服务端为什么没有调用`close`。