

# 服务端挂了，客户端的 TCP 连接还在吗？
如果「服务端挂掉」指的是「服务端进程崩溃」，那么服务端的进程在发生崩溃的时候，内核会发送 FIN 报文，与客户端进行四次挥手。

但是，如果「服务端挂掉」指的是「服务端主机宕机」，那么是不会发生四次挥手的，具体后续会发生什么？还要看客户端会不会发送数据？
* 如果客户端会发送数据，由于服务端已经不存在，客户端的数据报文会超时重传，当重传次数达到一定阈值后，会断开 TCP 连接；
* 如果客户端一直不会发送数据，再看客户端有没有开启 TCP keepalive 机制？
 * 如果有开启，客户端在一段时间后，检测到服务端的 TCP 连接已经不存在，则会断开自身的 TCP 连接；
 * 如果没有开启，客户端的 TCP 连接会一直存在，并不会断开。

## 服务端进程崩溃，客户端会发生什么？
TCP 的连接信息是由内核维护的，所以当服务端的进程崩溃后，内核需要回收该进程的所有 TCP 连接资源，于是内核会发送第一次挥手 FIN 报文，后续的挥手过程也都是在内核完成，并不需要进程的参与，所以即使服务端的进程退出了，还是能与客户端完成 TCP 四次挥手的过程。

使用 kill -9 命令来模拟进程崩溃的情况，发现在 kill 掉进程后，服务端会发送 FIN 报文，与客户端进行四次挥手。
## 服务端主机宕机后，客户端会发生什么？
当服务端的主机突然断电了，这种情况就是属于服务端主机宕机了。

当服务端的主机发生了宕机，是没办法和客户端进行四次挥手的，所以在服务端主机发生宕机的那一时刻，客户端是没办法立刻感知到服务端主机宕机了，只能在后续的数据交互中来感知服务端的连接已经不存在了。

因此，我们要分三种情况来讨论：
* 服务端主机宕机后，客户端会发送数据；
* 服务端主机宕机后，客户端一直不会发送数据；
* 服务端主机宕机后，然后马上重启了服务端，重启完成后，如果这时客户端发送了数据

### 服务端主机宕机后，如果客户端会发送数据
在服务端主机宕机后，客户端发送了数据报文，由于得不到响应，在等待一定时长后，客户端就会触发超时重传机制，重传未得到响应的数据报文。

当重传次数达到达到一定阈值后，内核就会判定出该 TCP 连接有问题，然后通过`Socket`接口告诉应用程序该 TCP 连接出问题了，于是客户端的 TCP 连接就会断开。

那 TCP 的数据报文具体重传几次呢？

在 Linux 系统中，提供了一个叫`tcp_retries2`配置项，默认值是 15：
```bash
[root@localhost ~]# cat /proc/sys/net/ipv4/tcp_retries2
15
```
这个内核参数是控制在 TCP 连接建立的情况下，超时重传的最大次数。

不过`tcp_retries2`设置了 15 次，并不代表 TCP 超时重传了 15 次才会通知应用程序终止该 TCP 连接，内核会根据`tcp_retries2`设置的值，计算出一个`timeout`（如果`tcp_retries2=15`，那么计算得到的`timeout = 924600 ms`），如果重传间隔超过这个`timeout`，则认为超过了阈值，就会停止重传，然后就会断开 TCP 连接。

在发生超时重传的过程中，每一轮的超时时间（RTO）都是倍数增长的，比如如果第一轮 RTO 是 200ms，那么第二轮 RTO 是 400ms，第三轮 RTO 是 800ms，以此类推。

而 RTO 是基于 RTT（一个包的往返时间） 来计算的，如果 RTT 较大，那么计算出来的 RTO 就越大，那么经过几轮重传后，很快就达到了上面的`timeout`值了。

举个例子，如果`tcp_retries2=15`，那么计算得到的`timeout = 924600 ms`，如果重传总间隔时长达到了`timeout`就会停止重传，然后就会断开 TCP 连接：
* 如果 RTT 比较小，那么 RTO 初始值就约等于下限 200ms，也就是第一轮的超时时间是 200ms，由于`timeout`总时长是 924600ms，表现出来的现象刚好就是重传了 15 次，超过了`timeout`值，从而断开 TCP 连接
* 如果 RTT 比较大，假设 RTO 初始值计算得到的是 1000ms，也就是第一轮的超时时间是 1 秒，那么根本不需要重传 15 次，重传总间隔就会超过 924600ms。

最小 RTO 和最大 RTO 是在 Linux 内核中定义好了：

```
#define TCP_RTO_MAX ((unsigned)(120*HZ))
#define TCP_RTO_MIN ((unsigned)(HZ/5))
```
Linux 2.6+ 使用 1000 毫秒的 HZ，因此`TCP_RTO_MIN`约为 200 毫秒，`TCP_RTO_MAX`约为 120 秒。

如果`tcp_retries`设置为 15，且 RTT 比较小，那么 RTO 初始值就约等于下限 200ms，这意味着它需要 924.6 秒才能将断开的 TCP 连接通知给上层（即应用程序），每一轮的 RTO 增长关系如下表格：

{% asset_img 1.png %}

### 服务端主机宕机后，如果客户端一直不发数据
在服务端主机发送宕机后，如果客户端一直不发送数据，那么还得看是否开启了`TCP keepalive`机制（TCP 保活机制）。

如果没有开启`TCP keepalive`机制，在服务端主机发送宕机后，如果客户端一直不发送数据，那么客户端的 TCP 连接将一直保持存在，所以我们可以得知一个点，在没有使用 TCP 保活机制，且双方不传输数据的情况下，一方的 TCP 连接处在`ESTABLISHED`状态时，并不代表另一方的 TCP 连接还一定是正常的。

而如果开启了`TCP keepalive`机制，在服务端主机发送宕机后，即使客户端一直不发送数据，在持续一段时间后，TCP 就会发送探测报文，探测服务端是否存活：
* 如果对端是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。
* 如果对端主机崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。

所以，`TCP keepalive`机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活。

{% asset_img 2.png %}

`TCP keepalive`机制机制的原理是这样的：

定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。

在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：
```
net.ipv4.tcp_keepalive_time=7200
net.ipv4.tcp_keepalive_intvl=75  
net.ipv4.tcp_keepalive_probes=9
```
每个参数的意思，具体如下：
`tcp_keepalive_time=7200`：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制
`tcp_keepalive_intvl=75`：表示每次检测间隔 75 秒；
`tcp_keepalive_probes=9`：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。

也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。

{% asset_img 3.png %}

注意，应用程序如果想使用 TCP 保活机制，需要通过`socket`接口设置`SO_KEEPALIVE`选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。

`TCP keepalive`机制探测的时间是有点长。

`TCP keepalive`是 TCP 层（内核态） 实现的，它是给所有基于 TCP 传输协议的程序一个兜底的方案。

实际上，我们应用层可以自己实现一套探测机制，可以在较短的时间内，探测到对方是否存活。

比如，web 服务软件一般都会提供`keepalive_timeout`参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会启动一个定时器，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，定时器的时间一到，就会触发回调函数来释放该连接。

{% asset_img 4.png %}

### 服务端主机宕机后，然后马上重启了服务端，客户端发送了数据
如果服务端主机宕机后，然后马上重启了服务端，重启完成后，如果这时客户端发送了数据，由于服务端之前的连接信息已经不存在，所以会回`RST`报文给客户端，客户端收到`RST`报文后，就断开连接。
## 总结
如果「服务端挂掉」指的是「服务端进程崩溃」，服务端的进程在发生崩溃的时候，内核会发送`FIN`报文，与客户端进行四次挥手。

但是，如果「服务端挂掉」指的是「服务端主机宕机」，那么是不会发生四次挥手的，具体后续会发生什么？还要看客户端会不会发送数据？
* 如果客户端会发送数据，由于服务端已经不存在，客户端的数据报文会超时重传，当重传总间隔时长达到一定阈值（内核会根据`tcp_retries2`设置的值计算出一个阈值）后，会断开 TCP 连接；
* 如果客户端一直不会发送数据，再看客户端有没有开启`TCP keepalive`机制？
 * 如果有开启，客户端在一段时间没有进行数据交互时，会触发`TCP keepalive`机制，探测对方是否存在，如果探测到对方已经消亡，则会断开自身的 TCP 连接；
 * 如果没有开启，客户端的 TCP 连接会一直存在，并且一直保持在`ESTABLISHED`状态。

# SYN 报文什么时候情况下会被丢弃？
客户端向服务端发起了连接，但是连接并没有建立起来，通过抓包分析发现，服务端是收到 SYN 报文了，但是并没有回复 SYN+ACK（TCP 第二次握手），说明 SYN 报文被服务端忽略了，然后客户端就一直在超时重传 SYN 报文，直到达到最大的重传次数。

接下来，我就给出我遇到过 SYN 报文被丢弃的两种场景：
* 开启 tcp_tw_recycle 参数，并且在 NAT 环境下，造成 SYN 报文被丢弃
* accept 队列满了，造成 SYN 报文被丢弃

## tcp_tw_recycle
TCP 四次挥手过程中，主动断开连接方会有一个 TIME_WAIT 的状态，这个状态会持续 2 MSL 后才会转变为 CLOSED 状态。
在 Linux  操作系统下，TIME_WAIT 状态的持续时间是 60 秒，这意味着这 60 秒内，客户端一直会占用着这个端口。要知道，端口资源也是有限的，一般可以开启的端口为 32768~61000 ，也可以通过如下参数设置指定范围：
```
net.ipv4.ip_local_port_range
```
那么，如果如果主动断开连接方的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。

但是 TIME_WAIT 状态也不是摆设作用，它的作用有两个：

防止具有相同四元组的旧数据包被收到，也就是防止历史连接中的数据，被后面的连接接受，否则就会导致后面的连接收到一个无效的数据，

保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭;

不过，Linux 操作系统提供了两个可以系统参数来快速回收处于 TIME_WAIT 状态的连接，这两个参数都是默认关闭的：

net.ipv4.tcp_tw_reuse，如果开启该选项的话，客户端（连接发起方） 在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用，所以该选项只适用于连接发起方。

net.ipv4.tcp_tw_recycle，如果开启该选项的话，允许处于 TIME_WAIT 状态的连接被快速回收；

要使得这两个选项生效，有一个前提条件，就是要打开 TCP 时间戳，即 net.ipv4.tcp_timestamps=1（默认即为 1）。

但是，tcp_tw_recycle 在使用了 NAT 的网络下是不安全的！

对于服务器来说，如果同时开启了 recycle 和 timestamps 选项，则会开启一种称之为「 per-host 的 PAWS 机制」。

首先给大家说说什么是  PAWS 机制？

tcp_timestamps 选项开启之后， PAWS 机制会自动开启，它的作用是防止 TCP 包中的序列号发生绕回。

正常来说每个 TCP 包都会有自己唯一的 SEQ，出现 TCP 数据包重传的时候会复用 SEQ 号，这样接收方能通过 SEQ 号来判断数据包的唯一性，也能在重复收到某个数据包的时候判断数据是不是重传的。但是 TCP 这个 SEQ 号是有限的，一共 32 bit，SEQ 开始是递增，溢出之后从 0 开始再次依次递增。

所以当 SEQ 号出现溢出后单纯通过 SEQ 号无法标识数据包的唯一性，某个数据包延迟或因重发而延迟时可能导致连接传递的数据被破坏，比如：

{% asset_img 11.png %}

上图 A 数据包出现了重传，并在 SEQ 号耗尽再次从 A 递增时，第一次发的 A 数据包延迟到达了 Server，这种情况下如果没有别的机制来保证，Server 会认为延迟到达的 A 数据包是正确的而接收，反而是将正常的第三次发的 SEQ 为 A 的数据包丢弃，造成数据传输错误。

PAWS 就是为了避免这个问题而产生的，在开启 tcp_timestamps 选项情况下，一台机器发的所有 TCP 包都会带上发送时的时间戳，PAWS 要求连接双方维护最近一次收到的数据包的时间戳（Recent TSval），每收到一个新数据包都会读取数据包中的时间戳值跟 Recent TSval 值做比较，如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包。

对于上面图中的例子有了 PAWS 机制就能做到在收到 Delay 到达的 A 号数据包时，识别出它是个过期的数据包而将其丢掉。

那什么是 per-host 的 PAWS 机制呢？

前面我提到，开启了 recycle 和 timestamps 选项，就会开启一种叫 per-host 的 PAWS 机制。

per-host 是对「对端 IP 做 PAWS 检查」，而非对「IP + 端口」四元组做 PAWS 检查。

但是如果客户端网络环境是用了 NAT 网关，那么客户端环境的每一台机器通过 NAT 网关后，都会是相同的 IP 地址，在服务端看来，就好像只是在跟一个客户端打交道一样，无法区分出来。

Per-host PAWS 机制利用TCP option里的 timestamp 字段的增长来判断串扰数据，而 timestamp 是根据客户端各自的 CPU tick 得出的值。

当客户端 A 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME-WAIT 状态的连接后，客户端 B 也通过 NAT 网关和服务器建立 TCP 连接，注意客户端 A  和 客户端 B 因为经过相同的 NAT 网关，所以是用相同的 IP 地址与服务端建立 TCP 连接，如果客户端 B 的 timestamp 比 客户端 A 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 B 发来的 SYN 包。

因此，tcp_tw_recycle 在使用了 NAT 的网络下是存在问题的，如果它是对 TCP 四元组做 PAWS 检查，而不是对「相同的 IP 做 PAWS 检查」，那么就不会存在这个问题了。

tcp_tw_recycle 在 Linux 4.12 版本后，直接取消了这一参数。
## accept 队列满了
在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是：
* 半连接队列，也称 SYN 队列；
* 全连接队列，也称 accepet 队列；

服务端收到客户端发起的 SYN 请求后，内核会把该连接存储到半连接队列，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。

{% asset_img 12.png %}

在服务端并发处理大量请求时，如果 TCP accept 队列过小，或者应用程序调用 accept() 不及时，就会造成 accept 队列满了 ，这时后续的连接就会被丢弃，这样就会出现服务端请求数量上不去的现象。

{% asset_img 13.png %}

我们可以通过 ss 命令来看 accept 队列大小，在「LISTEN 状态」时，Recv-Q/Send-Q 表示的含义如下：

{% asset_img 14.png %}

Recv-Q：当前 accept 队列的大小，也就是当前已完成三次握手并等待服务端 accept() 的 TCP 连接个数；

Send-Q：当前 accept 最大队列长度，上面的输出结果说明监听 8088 端口的 TCP 服务进程，accept 队列的最大长度为 128；

如果 Recv-Q 的大小超过 Send-Q，就说明发生了 accept 队列满的情况。

要解决这个问题，我们可以：
* 调大 accept 队列的最大长度，调大的方式是通过调大 backlog 以及 somaxconn 参数。
* 检查系统或者代码为什么调用 accept()  不及时；

# 什么是 TIME_WAIT 状态？
TCP 四次挥手过程，如下图：

{% asset_img 31.png %}

* 客户端打算关闭连接，此时会发送一个 TCP 首部`FIN`标志位被置为 1 的报文，也即`FIN`报文，之后客户端进入`FIN_WAIT_1`状态。
* 服务端收到该报文后，就向客户端发送`ACK`应答报文，接着服务端进入`CLOSED_WAIT`状态。
* 客户端收到服务端的`ACK`应答报文后，之后进入`FIN_WAIT_2`状态。
* 等待服务端处理完数据后，也向客户端发送`FIN`报文，之后服务端进入`LAST_ACK`状态。
* 客户端收到服务端的`FIN`报文后，回一个`ACK`应答报文，之后进入`TIME_WAIT`状态
* 服务器收到了`ACK`应答报文后，就进入了`CLOSE`状态，至此服务端已经完成连接的关闭。
* 客户端在经过 2MSL 一段时间后，自动进入 CLOSE 状态，至此客户端也完成连接的关闭。

可以看到，两个方向都需要一个`FIN`和一个`ACK`，因此通常被称为四次挥手。

这里一点需要注意是：主动关闭连接的，才有`TIME_WAIT`状态。

可以看到，`TIME_WAIT`是「主动关闭方」断开连接时的最后一个状态，该状态会持续 2MSL(`Maximum Segment Lifetime`) 时长，之后进入`CLOSED`状态。

MSL 指的是 TCP 协议中任何报文在网络上最大的生存时间，任何超过这个时间的数据都将被丢弃。虽然 RFC 793 规定 MSL 为 2 分钟，但是在实际实现的时候会有所不同，比如 Linux 默认为 30 秒，那么 2MSL 就是 60 秒。

MSL 是由网络层的 IP 包中的 TTL 来保证的，TTL 是 IP 头部的一个字段，用于设置一个数据报可经过的路由器的数量上限。报文每经过一次路由器的转发，IP 头部的 TTL 字段就会减 1，减到 0 时报文就被丢弃。

MSL 与 TTL 的区别：MSL 的单位是时间，而 TTL 是经过路由跳数。所以 MSL 应该要大于等于 TTL 消耗为 0 的时间，以确保报文已被自然消亡。

TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了。
## 为什么要设计 TIME_WAIT 状态？
设计`TIME_WAIT`状态，主要有两个原因：
* 防止历史连接中的数据，被后面相同四元组的连接错误的接收；
* 保证「被动关闭连接」的一方，能被正确的关闭；

### 原因一：防止历史连接中的数据，被后面相同四元组的连接错误的接收
为了能更好的理解这个原因，我们先来了解序列号（SEQ）和初始序列号（ISN）。

序列号，是 TCP 一个头部字段，标识了 TCP 发送端到 TCP 接收端的数据流的一个字节，因为 TCP 是面向字节流的可靠协议，为了保证消息的顺序性和可靠性，TCP 为每个传输方向上的每个字节都赋予了一个编号，以便于传输成功后确认、丢失后重传以及在接收端保证不会乱序。序列号是一个 32 位的无符号数，因此在到达 4G 之后再循环回到 0。

初始序列号，在 TCP 建立连接的时候，客户端和服务端都会各自生成一个初始序列号，它是基于时钟生成的一个随机数，来保证每个连接都拥有不同的初始序列号。初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时。

序列号和初始化序列号并不是无限递增的，会发生回绕为初始值的情况，这意味着无法根据序列号来判断新老数据。

假设`TIME-WAIT`没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？

{% asset_img 32.png %}

服务端在关闭连接之前发送的`SEQ = 301`报文，被网络延迟了。

接着，服务端以相同的四元组重新打开了新连接，前面被延迟的`SEQ = 301`这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，因此客户端会正常接收这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。

为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了`TIME_WAIT`状态，状态会持续 2MSL 时长，这个时间足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。
### 原因二：保证「被动关闭连接」的一方，能被正确的关闭
如果客户端（主动关闭方）最后一次`ACK`报文（第四次挥手）在网络中丢失了，那么按照 TCP 可靠性原则，服务端（被动关闭方）会重发`FIN`报文。

假设客户端没有`TIME_WAIT`状态，而是在发完最后一次回`ACK`报文就直接进入`CLOSED`状态，如果该`ACK`报文丢失了，服务端则重传的`FIN`报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的`FIN`报文后，就会回`RST`报文。

{% asset_img 33.png %}

服务端收到这个`RST`并将其解释为一个错误，这对于一个可靠的协议来说不是一个优雅的终止方式。

为了防止这种情况出现，客户端必须等待足够长的时间确保对端收到`ACK`，如果对端没有收到`ACK`，那么就会触发 TCP 重传机制，服务端会重新发送一个`FIN`，这样一去一来刚好两个 MSL 的时间。

{% asset_img 34.png %}

但是你可能会说重新发送的`ACK`还是有可能丢失啊，没错，但 TCP 已经等待了那么长的时间了，已经算仁至义尽了。
## tcp_tw_reuse 是什么？
在 Linux  操作系统下，`TIME_WAIT`状态的持续时间是 60 秒，这意味着这 60 秒内，客户端一直会占用着这个端口。要知道，端口资源也是有限的，一般可以开启的端口为 32768~61000 ，也可以通过如下参数设置指定范围：
```
net.ipv4.ip_local_port_range
```
那么，如果如果主动关闭连接方的`TIME_WAIT`状态过多，占满了所有端口资源，则会导致无法创建新连接。

不过，Linux 操作系统提供了两个可以系统参数来快速回收处于`TIME_WAIT`状态的连接，这两个参数都是默认关闭的：

`net.ipv4.tcp_tw_reuse`，如果开启该选项的话，客户端（连接发起方） 在调用`connect()`函数时，内核会随机找一个`TIME_WAIT`状态超过 1 秒的连接给新的连接复用，所以该选项只适用于连接发起方。

`net.ipv4.tcp_tw_recycle`，如果开启该选项的话，允许处于`TIME_WAIT`状态的连接被快速回收，该参数在 NAT 的网络下是不安全的！

要使得上面这两个参数生效，有一个前提条件，就是要打开 TCP 时间戳，即`net.ipv4.tcp_timestamps=1`（默认即为 1）。

开启了`tcp_timestamps`参数，TCP 头部就会使用时间戳选项，它有两个好处，一个是便于精确计算 RTT ，另一个是能防止序列号回绕（PAWS），我们先来介绍这个功能。

序列号是一个 32 位的无符号整型，上限值是 4GB，超过 4GB 后就需要将序列号回绕进行重用。这在以前网速慢的年代不会造成什么问题，但在一个速度足够快的网络中传输大量数据时，序列号的回绕时间就会变短。如果序列号回绕的时间极短，我们就会再次面临之前延迟的报文抵达后序列号依然有效的问题。

为了解决这个问题，就需要有 TCP 时间戳。

试看下面的示例，假设 TCP 的发送窗口是 1 GB，并且使用了时间戳选项，发送方会为每个 TCP 报文分配时间戳数值，我们假设每个报文时间加 1，然后使用这个连接传输一个 6GB 大小的数据流。

{% asset_img 35.png %}

32 位的序列号在时刻 D 和 E 之间回绕。假设在时刻B有一个报文丢失并被重传，又假设这个报文段在网络上绕了远路并在时刻 F 重新出现。如果 TCP 无法识别这个绕回的报文，那么数据完整性就会遭到破坏。

使用时间戳选项能够有效的防止上述问题，如果丢失的报文会在时刻 F 重新出现，由于它的时间戳为 2，小于最近的有效时间戳（5 或 6），因此防回绕序列号算法（PAWS）会将其丢弃。

防回绕序列号算法要求连接双方维护最近一次收到的数据包的时间戳（`Recent TSval`），每收到一个新数据包都会读取数据包中的时间戳值跟`Recent TSval`值做比较，如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包。
## 为什么 tcp_tw_reuse  默认是关闭的？
开启`tcp_tw_reuse`会有什么风险呢？我觉得会有 2 个问题。

### 第一个问题
我们知道开启`tcp_tw_reuse`的同时，也需要开启`tcp_timestamps`，意味着可以用时间戳的方式有效的判断回绕序列号的历史报文。

但是在看我看了防回绕序列号算法的源码后，发现对于`RST`报文的时间戳即使过期了，只要`RST`报文的序列号在对方的接收窗口内，也是能被接受的。

下面`tcp_validate_incoming`函数就是验证接收到的 TCP 报文是否合格的函数，其中第一步就会进行 PAWS 检查，由`tcp_paws_discard`函数负责。
```
static bool tcp_validate_incoming(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th, int syn_inerr)
{
  struct tcp_sock *tp = tcp_sk(sk);

  /* RFC1323: H1. Apply PAWS check first. */
  if (tcp_fast_parse_options(sock_net(sk), skb, th, tp) &&
      tp->rx_opt.saw_tstamp &&
      tcp_paws_discard(sk, skb)) {
      if (!th->rst) {
          ....
          goto discard;
      }
      /* Reset is accepted even if it did not pass PAWS. */
  }
}
```
当`tcp_paws_discard`返回`true`，就代表报文是一个历史报文，于是就要丢弃这个报文。但是在丢掉这个报文的时候，会先判断是不是`RST`报文，如果不是`RST`报文，才会将报文丢掉。也就是说，即使`RST`报文是一个历史报文，并不会被丢弃。

假设有这样的场景，如下图：

{% asset_img 36.png %}

客户端向一个还没有被服务端监听的端口发起了 HTTP 请求，接着服务端就会回`RST`报文给对方，很可惜的是`RST`报文被网络阻塞了。

由于客户端迟迟没有收到 TCP 第二次握手，于是重发了 SYN 包，与此同时服务端已经开启了服务，监听了对应的端口。于是接下来，客户端和服务端就进行了 TCP 三次握手、数据传输（HTTP应答-响应）、四次挥手。

因为客户端开启了`tcp_tw_reuse`，于是快速复用`TIME_WAIT`状态的端口，又与服务端建立了一个与刚才相同的四元组的连接。

接着，前面被网络延迟`RST`报文这时抵达了客户端，而且`RST`报文的序列号在客户端的接收窗口内，由于防回绕序列号算法不会防止过期的`RST`，所以`RST`报文会被客户端接受了，于是客户端的连接就断开了。

上面这个场景就是开启`tcp_tw_reuse`风险，因为快速复用`TIME_WAIT`状态的端口，导致新连接可能被回绕序列号的`RST`报文断开了，而如果不跳过`TIME_WAIT`状态，而是停留 2MSL 时长，那么这个`RST`报文就不会出现下一个新的连接。

可能大家会有这样的疑问，为什么 PAWS 检查要放过过期的`RST`报文。我翻了 RFC 1323 ，里面有一句提到：

大概的意思：建议`RST`段不携带时间戳，并且无论其时间戳如何，`RST`段都是可接受的。老的重复的`RST`段应该是极不可能的，并且它们的清除功能应优先于时间戳。

RFC 1323 提到说收历史的`RST`报文是极不可能，之所以有这样的想法是因为`TIME_WAIT`状态持续的 2MSL 时间，足以让连接中的报文在网络中自然消失，所以认为按正常操作来说是不会发生的，因此认为清除连接优先于时间戳。

而我前面提到的案例，是因为开启了`tcp_tw_reuse`状态，跳过了`TIME_WAIT`状态，才发生的事情。

有同学会说，都经过一个 HTTP 请求了，延迟的`RST`报文竟然还会存活？

一个 HTTP 请求其实很快的，比如我下面这个抓包，只需要 0.2 秒就完成了，远小于 MSL，所以延迟的`RST`报文存活是有可能的。

{% asset_img 37.png %}

### 第二个问题
开启`tcp_tw_reuse`来快速复用`TIME_WAIT`状态的连接，如果第四次挥手的`ACK`报文丢失了，有可能会导致被动关闭连接的一方不能被正常的关闭，如下图：

{% asset_img 38.png %}

## 总结
`tcp_tw_reuse`的作用是让客户端快速复用处于`TIME_WAIT`状态的端口，相当于跳过了`TIME_WAIT`状态，这可能会出现这样的两个问题：
* 历史`RST`报文可能会终止后面相同四元组的连接，因为 PAWS 检查到即使`RST`是过期的，也不会丢弃。
* 如果第四次挥手的`ACK`报文丢失了，有可能被动关闭连接的一方不能被正常的关闭;

虽然`TIME_WAIT`状态持续的时间是有一点长，显得很不友好，但是它被设计来就是用来避免发生乱七八糟的事情。

# 拔掉网线后， 原本的 TCP 连接还存在吗？
TCP 连接在 Linux 内核中是一个名为 struct socket 的结构体，该结构体的内容包含 TCP 连接的状态等信息。当拔掉网线的时候，操作系统并不会变更该结构体的任何内容，所以 TCP 连接的状态也不会发生改变。

拔掉网线这个动作并不会影响 TCP 连接的状态。

接下来，要看拔掉网线后，双方做了什么动作。

所以， 针对这个问题，要分场景来讨论：

拔掉网线后，有数据传输；
拔掉网线后，没有数据传输；

拔掉网线后，有数据传输
在客户端拔掉网线后，服务端向客户端发送的数据报文会得不到任何的响应，在等待一定时长后，服务端就会触发超时重传机制，重传未得到响应的数据报文。

如果在服务端重传报文的过程中，客户端刚好把网线插回去了，由于拔掉网线并不会改变客户端的 TCP 连接状态，并且还是处于 ESTABLISHED 状态，所以这时客户端是可以正常接收服务端发来的数据报文的，然后客户端就会回 ACK 响应报文。

此时，客户端和服务端的 TCP 连接依然存在的，就感觉什么事情都没有发生。

但是，如果如果在服务端重传报文的过程中，客户端一直没有将网线插回去，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了，于是服务端的 TCP 连接就会断开。

而等客户端插回网线后，如果客户端向服务端发送了数据，由于服务端已经没有与客户端相同四元祖的 TCP 连接了，因此服务端内核就会回复 RST 报文，客户端收到后就会释放该 TCP 连接。

此时，客户端和服务端的 TCP 连接都已经断开了。

那 TCP 的数据报文具体重传几次呢？

在 Linux 系统中，提供了一个叫 tcp_retries2 配置项，默认值是 15，如下图：
```bash
[root@localhost ~]# cat /proc/sys/net/ipv4/tcp_retries2
15
```
这个内核参数是控制，在 TCP 连接建立的情况下，超时重传的最大次数。

不过 tcp_retries2 设置了 15 次，并不代表 TCP 超时重传了 15 次才会通知应用程序终止该 TCP 连接，内核还会基于「最大超时时间」来判定。

每一轮的超时时间都是倍数增长的，比如第一次触发超时重传是在 2s 后，第二次则是在 4s 后，第三次则是 8s 后，以此类推。

{% asset_img 1.png %}

内核会根据 tcp_retries2 设置的值，计算出一个最大超时时间。

在重传报文且一直没有收到对方响应的情况时，先达到「最大重传次数」或者「最大超时时间」这两个的其中一个条件后，就会停止重传，然后就会断开 TCP 连接。
## 拔掉网线后，没有数据传输
针对拔掉网线后，没有数据传输的场景，还得看是否开启了 TCP keepalive 机制 （TCP 保活机制）。

如果没有开启 TCP keepalive 机制，在客户端拔掉网线后，并且双方都没有进行数据传输，那么客户端和服务端的 TCP 连接将会一直保持存在。

而如果开启了 TCP keepalive 机制，在客户端拔掉网线后，即使双方都没有进行数据传输，在持续一段时间后，TCP 就会发送探测报文：

如果对端是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。
如果对端主机崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。
所以，TCP 保活机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活。

TCP keepalive 机制具体是怎么样的？

这个机制的原理是这样的：

定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。

在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：
```
net.ipv4.tcp_keepalive_time=7200
net.ipv4.tcp_keepalive_intvl=75  
net.ipv4.tcp_keepalive_probes=9
```
tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制
tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；
tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。
也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。

{% asset_img 41.png %}

注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 SO_KEEPALIVE 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。

TCP keepalive 机制探测的时间也太长了吧？

对的，是有点长。

TCP keepalive  是 TCP 层（内核态） 实现的，它是给所有基于 TCP 传输协议的程序一个兜底的方案。

实际上，我们应用层可以自己实现一套探测机制，可以在较短的时间内，探测到对方是否存活。

比如，web 服务软件一般都会提供 keepalive_timeout 参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会启动一个定时器，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，定时器的时间一到，就会触发回调函数来释放该连接。

{% asset_img 42.png %}

## 总结
客户端拔掉网线后，并不会直接影响 TCP 连接状态。所以，拔掉网线后，TCP 连接是否还会存在，关键要看拔掉网线之后，有没有进行数据传输。

有数据传输的情况：

在客户端拔掉网线后，如果服务端发送了数据报文，那么在服务端重传次数没有达到最大值之前，客户端就插回了网线，那么双方原本的 TCP 连接还是能正常存在，就好像什么事情都没有发生。

在客户端拔掉网线后，如果服务端发送了数据报文，在客户端插回网线之前，服务端重传次数达到了最大值时，服务端就会断开 TCP 连接。等到客户端插回网线后，向服务端发送了数据，因为服务端已经断开了与客户端相同四元组的 TCP 连接，所以就会回 RST 报文，客户端收到后就会断开 TCP 连接。至此， 双方的 TCP 连接都断开了。

没有数据传输的情况：

如果双方都没有开启 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，那么客户端和服务端的 TCP 连接状态将会一直保持存在。
如果双方都开启了 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，TCP keepalive 机制会探测到对方的 TCP 连接没有存活，于是就会断开 TCP 连接。而如果在 TCP 探测期间，客户端插回了网线，那么双方原本的 TCP 连接还是能正常存在。
除了客户端拔掉网线的场景，还有客户端「宕机和杀死进程」的两种场景。

第一个场景，客户端宕机这件事跟拔掉网线是一样无法被服务端的感知的，所以如果在没有数据传输，并且没有开启 TCP keepalive 机制时，，服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态，直到服务端重启进程。

所以，我们可以得知一个点。在没有使用 TCP 保活机制，且双方不传输数据的情况下，一方的 TCP 连接处在 ESTABLISHED 状态时，并不代表另一方的 TCP 连接还一定是正常的。

第二个场景，杀死客户端的进程后，客户端的内核就会向服务端发送 FIN 报文，与客户端进行四次挥手。

所以，即使没有开启 TCP keepalive，且双方也没有数据交互的情况下，如果其中一方的进程发生了崩溃，这个过程操作系统是可以感知的到的，于是就会发送 FIN 报文给对方，然后与对方进行 TCP 四次挥手。

