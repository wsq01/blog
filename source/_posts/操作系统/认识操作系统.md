---
title: 认识操作系统
date: 2021-12-28 19:13:32
tags: [计算机组成]
categories: 计算机组成
---

# 硬件结构
## 图灵机的⼯作⽅式
图灵的基本思想是⽤机器来模拟⼈们⽤纸笔进⾏数学运算的过程，⽽且还定义了计算机由哪些部分组成，程序⼜是如何执⾏的。

{% asset_img 1.png %}

图灵机的基本组成如下：
* 有⼀条「纸带」，纸带由⼀个个连续的格⼦组成，每个格⼦可以写⼊字符，纸带就好⽐内存，⽽纸带上的格⼦的字符就好⽐内存中的数据或程序；
* 有⼀个「读写头」，读写头可以读取纸带上任意格⼦的字符，也可以把字符写⼊到纸带的格⼦；
* 读写头上有⼀些部件，⽐如存储单元、控制单元以及运算单元：
 1. 存储单元⽤于存放数据；
 2. 控制单元⽤于识别字符是数据还是指令，以及控制程序的流程等；
 3. 运算单元⽤于执⾏运算指令；

知道了图灵机的组成后，我们以简单数学运算的`1+2`作为例⼦，来看看它是怎么执⾏这⾏代码的。

⾸先，⽤读写头把`1、2、+`这 3 个字符分别写⼊到纸带上的 3 个格⼦，然后读写头先停在 1 字符对应的格⼦上；

{% asset_img 2.png %}

接着，读写头读⼊ 1 到存储设备中，这个存储设备称为图灵机的状态；

{% asset_img 3.png %}

然后读写头向右移动⼀个格，⽤同样的⽅式把 2 读⼊到图灵机的状态，于是现在图灵机的状态中存储着两个连续的数字，1 和 2；

{% asset_img 4.png %}

读写头再往右移动⼀个格，就会碰到 + 号，读写头读到 + 号后，将 + 号传输给「控制单元」，控制单元发现是⼀个 + 号⽽不是数字，所以没有存⼊到状态中，因为 + 号是运算符指令，作⽤是加和⽬前的状态，于是通知「运算单元」⼯作。运算单元收到要加和状态中的值的通知后，就会把状态中的 1 和 2 读⼊并计算，再将计算的结果 3 存放到状态中；

{% asset_img 5.png %}

最后，运算单元将结果返回给控制单元，控制单元将结果传输给读写头，读写头向右移动，把结果 3 写⼊到纸带的格⼦中；

{% asset_img 6.png %}

通过上⾯的图灵机计算 1+2 的过程，可以发现图灵机主要功能就是读取纸带格⼦中的内容，然后交给控制单元识别字符是数字还是运算符指令，如果是数字则存⼊到图灵机状态中，如果是运算符，则通知运算符单元读取状态中的数值进⾏计算，计算结果最终返回给读写头，读写头把结果写⼊到纸带的格⼦中。

事实上，图灵机这个看起来很简单的⼯作⽅式，和我们今天的计算机是基本⼀样的。接下来，我们⼀同再看看当今计算机的组成以及⼯作⽅式。
## 冯诺依曼模型
在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了图灵机的设计，⽽且还提出⽤电⼦元件构造计算机，并约定了⽤⼆进制进⾏计算和存储，还定义计算机基本结构为 5 个部分，分别是中央处理器（CPU）、内存、输⼊设备、输出设备、总线。

{% asset_img 7.png %}

这 5 个部分也被称为冯诺依曼模型，接下来看看这 5 个部分的具体作⽤。
### 内存
我们的程序和数据都是存储在内存，存储的区域是线性的。

数据存储的单位是⼀个⼆进制位（`bit`），即 0 或 1。最⼩的存储单位是字节（`byte`），1 字节等于 8 位。内存的地址是从 0 开始编号的，然后⾃增排列，最后⼀个地址为内存总字节数 - 1，这种结构好似我们程序⾥的数组，所以内存读写任何⼀个数据的速度都是⼀样的。
### 中央处理器
中央处理器也就是我们常说的 CPU，32 位和 64 位 CPU 最主要区别在于⼀次能计算多少字节数据：
* 32 位 CPU ⼀次可以计算 4 个字节；
* 64 位 CPU ⼀次可以计算 8 个字节；

这⾥的 32 位和 64 位，通常称为 CPU 的位宽。

之所以 CPU 要这样设计，是为了能计算更⼤的数值，如果是 8 位的 CPU，那么⼀次只能计算 1 个字节 0~255 范围内的数值，这样就⽆法⼀次完成计算`10000*500`，于是为了能⼀次计算⼤数的运算，CPU 需要⽀持多个`byte`⼀起计算，所以 CPU 位宽越⼤，可以计算的数值就越⼤，⽐如说 32 位 CPU 能计算的最⼤整数是 4294967295 。

CPU 内部还有⼀些组件，常⻅的有寄存器、控制单元和逻辑运算单元等。其中，控制单元负责控制 CPU ⼯作，逻辑运算单元负责计算，⽽寄存器可以分为多种类，每种寄存器的功能⼜不尽相同。

CPU 中的寄存器主要作⽤是存储计算时的数据，为什么有了内存还需要寄存器？原因很简单，因为内存离 CPU 太远了，⽽寄存器就在 CPU ⾥，还紧挨着控制单元和逻辑运算单元，⾃然计算时速度会很快。

常⻅的寄存器种类：
* 通⽤寄存器，⽤来存放需要进⾏运算的数据，⽐如需要进⾏加和运算的两个数据。
* 程序计数器，⽤来存储 CPU 要执⾏下⼀条指令「所在的内存地址」，注意不是存储了下⼀条要执⾏的指令，此时指令还在内存中，程序计数器只是存储了下⼀条指令的地址。
* 指令寄存器，⽤来存放程序计数器指向的指令，也就是指令本身，指令被执⾏完成之前，指令都存储在这⾥。

### 总线
总线是⽤于 CPU 和内存以及其他设备之间的通信，总线可分为 3 种：
* 地址总线，⽤于指定 CPU 将要操作的内存地址；
* 数据总线，⽤于读写内存的数据；
* 控制总线，⽤于发送和接收信号，⽐如中断、设备复位等信号，CPU 收到信号后⾃然进⾏响应，这时也需要控制总线；

当 CPU 要读写内存数据的时候，⼀般需要通过两个总线：
* ⾸先要通过「地址总线」来指定内存的地址；
* 再通过「数据总线」来传输数据；

### 输⼊、输出设备
输⼊设备向计算机输⼊数据，计算机经过计算后，把数据输出给输出设备。期间，如果输⼊设备是键盘，按下按键时是需要和 CPU 进⾏交互的，这时就需要⽤到控制总线了。
## 线路位宽与 CPU 位宽
数据是如何通过线路传输的呢？其实是通过操作电压，低电压表示 0，⾼压电压则表示 1。

如果构造了⾼低⾼这样的信号，其实就是 101 ⼆进制数据，⼗进制则表示 5，如果只有⼀条线路，就意味着每次只能传递`1 bit`的数据，即 0 或 1，那么传输 101 这个数据，就需要 3 次才能传输完成，这样的效率⾮常低。

这样⼀位⼀位传输的⽅式，称为串⾏，下⼀个`bit`必须等待上⼀个`bit`传输完成才能进⾏传输。当然，想⼀次多传⼀些数据，增加线路即可，这时数据就可以并⾏传输。

为了避免低效率的串⾏传输的⽅式，线路的位宽最好⼀次就能访问到所有的内存地址。CPU 要想操作的内存地址就需要地址总线，如果地址总线只有 1 条，那每次只能表示 0 或 1 这两种情况，所以 CPU ⼀次只能操作 2 个内存地址，如果想要 CPU 操作 4G 的内存，那么就需要 32 条地址总线，因为 2<sup>32</sup> = 4G。

知道了线路位宽的意义后，我们再来看看 CPU 位宽。

CPU 的位宽最好不要⼩于线路位宽，⽐如 32 位 CPU 控制 40 位宽的地址总线和数据总线的话，⼯作起来就会⾮常复杂且麻烦，所以 32 位的 CPU 最好和 32 位宽的线路搭配，因为 32 位 CPU ⼀次最多只能操作 32 位宽的地址总线和数据总线。

如果⽤ 32 位 CPU 去加和两个 64 位⼤⼩的数字，就需要把这 2 个 64 位的数字分成 2 个低位 32 位数字和 2 个⾼位 32 位数字来计算，先加个两个低位的 32 位数字，算出进位，然后加和两个⾼位的 32 位数字，最后再加上进位，就能算出结果了，可以发现 32 位 CPU 并不能⼀次性计算出加和两个 64 位数字的结果。

对于 64 位 CPU 就可以⼀次性算出加和两个 64 位数字的结果，因为 64 位 CPU 可以⼀次读⼊ 64 位的数字，并且 64 位 CPU 内部的逻辑运算单元也⽀持 64 位数字的计算。

但是并不代表 64 位 CPU 性能⽐ 32 位 CPU ⾼很多，很少应⽤需要算超过 32 位的数字，所以如果计算的数额不超过 32 位数字的情况下，32 位和 64 位 CPU 之间没什么区别的，只有当计算超过 32 位数字的情况下，64 位的优势才能体现出来。

另外，32 位 CPU 最⼤只能操作 4GB 内存，就算装了 8GB 内存条也没⽤。⽽ 64 位 CPU 寻址范围则很⼤，理论最⼤的寻址空间为 2<sup>64</sup>。
## 程序执⾏的基本过程
在前⾯，我们知道了程序在图灵机的执⾏过程，接下来我们来看看程序在冯诺依曼模型上是怎么执⾏的。

程序实际上是⼀条⼀条指令，所以程序的运⾏过程就是把每⼀条指令⼀步⼀步的执⾏起来，负责执⾏指令的就是 CPU 了。

{% asset_img 8.png %}

CPU 执⾏程序的过程如下：
* 第⼀步，CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存⼊到「指令寄存器」。
* 第⼆步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执⾏；
* 第三步，CPU 执⾏完指令后，「程序计数器」的值⾃增，表示指向下⼀条指令。这个⾃增的⼤⼩，由 CPU 的位宽决定，⽐如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会⾃增 4；

简单总结⼀下就是，⼀个程序执⾏的时候，CPU 会根据程序计数器⾥的内存地址，从内存⾥⾯把需要执⾏的指令读取到指令寄存器⾥⾯执⾏，然后根据指令⻓度⾃增，开始顺序读取下⼀条指令。

CPU 从程序计数器读取指令、到执⾏、再到下⼀条指令，这个过程会不断循环，直到程序执⾏结束，这个不断循环的过程被称为 CPU 的指令周期。
## a=1+2 执⾏具体过程
知道了基本的程序执⾏过程后，接下来⽤`a =1+2`的作为例⼦，进⼀步分析该程序在冯诺伊曼模型的执⾏过程。

CPU 是不认识`a=1+2`这个字符串，这些字符串只是⽅便我们程序员认识，要想这段程序能跑起来，还需要把整个程序翻译成汇编语⾔的程序，这个过程称为编译成汇编代码。

针对汇编代码，我们还需要⽤汇编器翻译成机器码，这些机器码由 0 和 1 组成的机器语⾔，这⼀条条机器码，就是⼀条条的计算机指令，这个才是 CPU 能够真正认识的东⻄。

下⾯来看看`a=1+2`在 32 位 CPU 的执⾏过程。

程序编译过程中，编译器通过分析代码，发现 1 和 2 是数据，于是程序运⾏时，内存会有个专⻔的区域来存放这些数据，这个区域就是「数据段」。如下图，数据 1 和 2 的区域位置：
* 数据 1 被存放到`0x104`位置；
* 数据 2 被存放到`0x100`位置；

注意，数据和指令是分开区域存放的，存放指令区域的地⽅称为「正⽂段」。

{% asset_img 9.png %}

编译器会把`a=1+2`翻译成 4 条指令，存放到正⽂段中。如图，这 4 条指令被存放到了`0x200~0x20c`的区域中：
* `0x200`的内容是`load`指令将`0x100`地址中的数据 2 装⼊到寄存器 R0；
* `0x204`的内容是`load`指令将`0x104`地址中的数据 1 装⼊到寄存器 R1；
* `0x208`的内容是`add`指令将寄存器 R0 和 R1 的数据相加，并把结果存放到寄存器 R2；
* `0x20c`的内容是`store`指令将寄存器 R2 中的数据存回数据段中的`0x108`地址中，这个地址也就是变量`a`内存中的地址；

编译完成后，具体执⾏程序的时候，程序计数器会被设置为 0x200 地址，然后依次执⾏这 4 条指令。

上⾯的例⼦中，由于是在 32 位 CPU 执⾏的，因此⼀条指令是占 32 位⼤⼩，所以你会发现每条指令间隔 4 个字节。

⽽数据的⼤⼩是根据你在程序中指定的变量类型，⽐如`int`类型的数据则占 4 个字节，`char`类型的数据则占 1 个字节。
### 指令
上⾯的例⼦中，图中指令的内容是简易的汇编代码，⽬的是为了⽅便理解指令的具体内容，事实上指令的内容是⼀串⼆进制数字的机器码，每条指令都有对应的机器码，CPU 通过解析机器码来知道指令的内容。

不同的 CPU 有不同的指令集，也就是对应着不同的汇编语⾔和不同的机器码，接下来选⽤最简单的 MIPS 指集，来看看机器码是如何⽣成的，这样也能明⽩⼆进制的机器码的具体含义。

MIPS 的指令是⼀个 32 位的整数，⾼ 6 位代表着操作码，表示这条指令是⼀条什么样的指令，剩下的 26 位不同指令类型所表示的内容也就不相同，主要有三种类型R、I 和 J。

{% asset_img 10.png %}

这三种类型的含义：
* R 指令，⽤在算术和逻辑操作，⾥⾯由读取和写⼊数据的寄存器地址。如果是逻辑位移操作，后⾯还有位移操作的「位移量」，⽽最后的「功能码」则是再前⾯的操作码不够的时候，扩展操作码来表示对应的具体指令的；
* I 指令，⽤在数据传输、条件分⽀等。这个类型的指令，就没有了位移量和操作码，也没有了第三个寄存器，⽽是把这三部分直接合并成了⼀个地址值或⼀个常数；
* J 指令，⽤在跳转，⾼ 6 位之外的 26 位都是⼀个跳转后的地址；

接下来，我们把前⾯例⼦的这条指令：「`add`指令将寄存器 R0 和 R1 的数据相加，并把结果放⼊到 R2」，翻译成机器码。

{% asset_img 11.png %}

加和运算`add`指令是属于 R 指令类型：
* `add`对应的 MIPS 指令⾥操作码是`000000`，以及最末尾的功能码是`100000`，这些数值都是固定的，查⼀下 MIPS 指令集的⼿册就能知道的；
* `rs`代表第⼀个寄存器 R0 的编号，即`00000`；
* `rt`代表第⼆个寄存器 R1 的编号，即`00001`；
* `rd`代表⽬标的临时寄存器 R2 的编号，即 00010；

因为不是位移操作，所以位移量是`00000`把上⾯这些数字拼在⼀起就是⼀条 32 位的 MIPS 加法指令了，那么⽤ 16 进制表示的机器码则是`0x00011020`。

编译器在编译程序的时候，会构造指令，这个过程叫做指令的编码。CPU 执⾏程序的时候，就会解析指令，这个过程叫作指令的解码。

现代⼤多数 CPU 都使⽤来流⽔线的⽅式来执⾏指令，所谓的流⽔线就是把⼀个任务拆分成多个⼩任务，于是⼀条指令通常分为 4 个阶段，称为 4 级流⽔线，如下图：

{% asset_img 12.png %}

四个阶段的具体含义：
1. CPU 通过程序计数器读取对应内存地址的指令，这个部分称为`Fetch`（取得指令）；
2. CPU 对指令进⾏解码，这个部分称为`Decode`（指令译码）；
3. CPU 执⾏指令，这个部分称为`Execution`（执⾏指令）；
4. CPU 将计算结果存回寄存器或者将寄存器的值存⼊内存，这个部分称为`Store`（数据回写）；

上⾯这 4 个阶段，我们称为指令周期，CPU 的⼯作就是⼀个周期接着⼀个周期，周⽽复始。

事实上，不同的阶段其实是由计算机中的不同组件完成的：
* 取指令的阶段，我们的指令是存放在存储器⾥的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由控制器操作的；
* 指令的译码过程，也是由控制器进⾏的；
* 指令执⾏的过程，⽆论是进⾏算术操作、逻辑操作，还是进⾏数据传输、条件分⽀操作，都是由算术逻辑单元操作的，也就是由运算器处理的。但是如果是⼀个简单的⽆条件地址跳转，则是直接在控制器⾥⾯完成的，不需要⽤到运算器。

{% asset_img 13.png %}

### 指令的类型
指令从功能⻆度划分，可以分为 5 ⼤类：
* 数据传输类型的指令，⽐如`store/load`是寄存器与内存间数据传输的指令，`mov`是将⼀个内存地址的数据移动到另⼀个内存地址的指令；
* 运算类型的指令，⽐如加减乘除、位运算、⽐较⼤⼩等等，它们最多只能处理两个寄存器中的数据；
* 跳转类型的指令，通过修改程序计数器的值来达到跳转执⾏指令的过程，⽐如编程中常⻅的`if else、swtich-case`、函数调⽤等。
* 信号类型的指令，⽐如发⽣中断的指令`trap`；
* 闲置类型的指令，⽐如指令`nop`，执⾏后 CPU 会空转⼀个周期；

### 指令的执⾏速度
CPU 的硬件参数都会有 GHz 这个参数，⽐如⼀个 1 GHz 的 CPU，指的是时钟频率是 1 G，代表着 1 秒会产⽣ 1G 次数的脉冲信号，每⼀次脉冲信号⾼低电平的转换就是⼀个周期，称为时钟周期。

对于 CPU 来说，在⼀个时钟周期内，CPU 仅能完成⼀个最基本的动作，时钟频率越⾼，时钟周期就越短，⼯作速度也就越快。

⼀个时钟周期⼀定能执⾏完⼀条指令吗？答案是不⼀定的，⼤多数指令不能在⼀个时钟周期完成，通常需要若⼲个时钟周期。不同的指令需要的时钟周期是不同的，加法和乘法都对应着⼀条 CPU 指令，但是乘法需要的时钟周期就要⽐加法多。

程序执⾏的时候，耗费的 CPU 时间少就说明程序是快的，对于程序的 CPU 执⾏时间，我们可以拆解成 CPU 时钟周期数（CPU Cycles）和时钟周期时间（Clock Cycle Time）的乘积。

{% asset_img 14.png %}

时钟周期时间就是我们前⾯提及的 CPU 主频，主频越⾼说明 CPU 的⼯作速度就越快，⽐如电脑的 CPU 是 2.4GHz 四核 Intel Core i5，这⾥的 2.4GHz 就是电脑的主频，时钟周期时间就是 1/2.4G。

要想 CPU 跑的更快，⾃然缩短时钟周期时间，也就是提升 CPU 主频，但是今⾮彼⽇，摩尔定律早已失效，当今的 CPU 主频已经很难再做到翻倍的效果了。

另外，换⼀个更好的 CPU，这个也是我们软件⼯程师控制不了的事情，我们应该把⽬光放到另外⼀个乘法因⼦ —— CPU 时钟周期数，如果能减少程序所需的 CPU 时钟周期数量，⼀样也是能提升程序的性能的。

对于 CPU 时钟周期数我们可以进⼀步拆解成：「指令数 x 每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI ）」，于是程序的 CPU 执⾏时间的公式可变成如下：

{% asset_img 15.png %}

因此，要想程序跑的更快，优化这三者即可：
* 指令数，表示执⾏程序所需要多少条指令，以及哪些指令。这个层⾯是基本靠编译器来优化，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示⽅式。
* 每条指令的平均时钟周期数 CPI，表示⼀条指令需要多少个时钟周期数，现代⼤多数 CPU 通过流⽔线技术（Pipline），让⼀条指令需要的 CPU 时钟周期数尽可能的少；
* 时钟周期时间，表示计算机主频，取决于计算机硬件。有的 CPU ⽀持超频技术，打开了超频意味着把 CPU 内部的时钟给调快了，于是 CPU ⼯作速度就变快了，但是也是有代价的，CPU 跑的越快，散热的压⼒就会越⼤，CPU 会很容易奔溃。

# 存储器
## 存储器的层次结构
我们可以把 CPU ⽐喻成我们的⼤脑，⼤脑正在思考的东⻄，就好⽐ CPU 中的寄存器，处理速度是最快的，但是能存储的数据也是最少的，毕竟我们也不能⼀下同时思考太多的事情。

我们⼤脑中的记忆，就好⽐ CPU Cache，中⽂称为 CPU ⾼速缓存，处理速度相⽐寄存器慢了⼀点，但是能存储的数据也稍微多了⼀些。

CPU Cache 通常会分为 L1、L2、L3 三层，其中 L1 Cache 通常分成「数据缓存」和「指令缓存」，L1是距离 CPU 最近的，因此它⽐ L2、L3 的读写速度都快、存储空间都⼩。我们⼤脑中短期记忆，就好⽐ L1 Cache，⽽⻓期记忆就好⽐ L2/L3 Cache。

寄存器和 CPU Cache 都是在 CPU 内部，跟 CPU 挨着很近，因此它们的读写速度都相当的快，但是能存储的数据很少，毕竟 CPU 就这么丁点⼤。

知道 CPU 内部的存储器的层次分布，我们放眼看看 CPU 外部的存储器。

当我们⼤脑记忆中没有资料的时候，可以从书桌或书架上拿书来阅读，那我们桌⼦上的书，就好⽐内存，我们虽然可以⼀伸⼿就可以拿到，但读写速度肯定远慢于寄存器，那图书馆书架上的书，就好⽐硬盘，能存储的数据⾮常⼤，但是读写速度相⽐内存差好⼏个数量级，更别说跟寄存器的差距了。

{% asset_img 16.png %}

我们从图书馆书架取书，把书放到桌⼦上，再阅读书，我们⼤脑就会记忆知识点，然后再经过⼤脑思考，这⼀系列过程相当于，数据从硬盘加载到内存，再从内存加载到 CPU 的寄存器和 Cache 中，然后再通过 CPU 进⾏处理和计算。

对于存储器，它的速度越快、能耗会越⾼、⽽且材料的成本也是越贵的，以⾄于速度快的存储器的容量都⽐较⼩。

CPU ⾥的寄存器和 Cache，是整个计算机存储器中价格最贵的，虽然存储空间很⼩，但是读写速度是极快的，⽽相对⽐较便宜的内存和硬盘，速度肯定⽐不上 CPU 内部的存储器，但是能弥补存储空间的不⾜。

存储器通常可以分为这么⼏个级别：

{% asset_img 17.png %}

### 寄存器
最靠近 CPU 的控制单元和逻辑计算单元的存储器，就是寄存器了，它的速度也是最快的，因此价格也是最贵的，那么数量不能很多。

存储器的数量通常在⼏⼗到⼏百之间，每个寄存器可以⽤来存储⼀定的字节的数据。⽐如：
* 32 位 CPU 中⼤多数寄存器可以存储 4 个字节；
* 64 位 CPU 中⼤多数寄存器可以存储 8 个字节。

寄存器的访问速度⾮常快，⼀般要求在半个 CPU 时钟周期内完成读写，CPU 时钟周期跟 CPU 主频息息相关，⽐如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）。

CPU 处理⼀条指令的时候，除了读写寄存器，还需要解码指令、控制指令执⾏和计算。如果寄存器的速度太慢，则会拉⻓指令的处理周期，从⽽给⽤户的感觉，就是电脑「很慢」。
### CPU Cache
CPU Cache ⽤的是⼀种叫 SRAM（Static Random-Access Memory，静态随机存储器） 的芯⽚。SRAM 之所以叫「静态」存储器，是因为只要有电，数据就可以保持存在，⽽⼀旦断电，数据就会丢失了。

在 SRAM ⾥⾯，⼀个`bit`的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不⾼，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度⾮常快。

CPU 的⾼速缓存，通常可以分为 L1、L2、L3 这样的三层⾼速缓存，也称为⼀级缓存、⼆次缓存、三次缓存。

{% asset_img 18.png %}

### L1 ⾼速缓存
L1 ⾼速缓存的访问速度⼏乎和寄存器⼀样快，通常只需要 2~4 个时钟周期，⽽⼤⼩在⼏⼗ KB 到⼏百 KB 不等。

每个 CPU 核⼼都有⼀块属于⾃⼰的 L1 ⾼速缓存，指令和数据在 L1 是分开存放的，所以 L1 ⾼速缓存通常分成指令缓存和数据缓存。
### L2 ⾼速缓存
L2 ⾼速缓存同样每个 CPU 核⼼都有，但是 L2 ⾼速缓存位置⽐ L1 ⾼速缓存距离 CPU 核⼼更远，它⼤⼩⽐ L1 ⾼速缓存更⼤，CPU 型号不同⼤⼩也就不同，通常⼤⼩在⼏百 KB 到⼏ MB 不等，访问速度则更慢，速度在 10~20 个时钟周期。
### L3 ⾼速缓存
L3 ⾼速缓存通常是多个 CPU 核⼼共⽤的，位置⽐ L2 ⾼速缓存距离 CPU 核⼼更远，⼤⼩也会更⼤些，通常⼤⼩在⼏ MB 到⼏⼗ MB 不等，具体值根据 CPU 型号⽽定。

访问速度相对也⽐较慢⼀些，访问速度在 20~60 个时钟周期。
### 内存
内存⽤的芯⽚和 CPU Cache 有所不同，它使⽤的是⼀种叫作 DRAM （Dynamic Random Access Memory，动态随机存取存储器） 的芯⽚。

相⽐ SRAM，DRAM 的密度更⾼，功耗更低，有更⼤的容量，⽽且造价⽐ SRAM 芯⽚便宜很多。

DRAM 存储⼀个`bit`数据，只需要⼀个晶体管和⼀个电容就能存储，但是因为数据会被存储在电容⾥，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。

DRAM 的数据访问电路和刷新电路都⽐ SRAM 更复杂，所以访问的速度会更慢，内存速度⼤概在 200~300 个 时钟周期之间。
### SSD/HDD 硬盘
SSD（Solid-state disk） 就是我们常说的固体硬盘，结构和内存类似，但是它相⽐内存的优点是断电后数据还是存在的，⽽内存、寄存器、⾼速缓存断电后数据都会丢失。内存的读写速度⽐ SSD ⼤概快 10~1000 倍。

当然，还有⼀款传统的硬盘，也就是机械硬盘（Hard Disk Drive, HDD），它是通过物理读写的⽅式来访问数据的，因此它访问速度是⾮常慢的，它的速度⽐内存慢 10W 倍左右。

由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。
## 存储器的层次关系
现代的⼀台计算机，都⽤上了 CPU Cahce、内存、到 SSD 或 HDD 硬盘这些存储器设备了。

其中，存储空间越⼤的存储器设备，其访问速度越慢，所需成本也相对越少。

CPU 并不会直接和每⼀种存储器设备直接打交道，⽽是每⼀种存储器设备只和它相邻的存储器设备打交道。

⽐如，CPU Cache 的数据是从内存加载过来的，写回数据的时候也只写回到内存，CPU Cache 不会直接把数据写到硬盘，也不会直接从硬盘加载数据，⽽是先加载到内存，再从内存加载到 CPU Cache 中。

{% asset_img 19.png %}

所以，每个存储器只和相邻的⼀层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是更⾼，也正因为成本太⾼，所以 CPU 内部的寄存器、L1\L2\L3 Cache 只好⽤较⼩的容量，相反内存、硬盘则可⽤更⼤的容量。

另外，当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即可，如果寄存器没有这个数据，CPU 就会查询 L1 ⾼速缓存，如果 L1 没有，则查询 L2 ⾼速缓存，L2 还是没有的话就查询 L3 ⾼速缓存，L3 依然没有的话，才去内存中取数据。

所以，存储层次结构也形成了缓存的体系。
# 为什么 0.1 + 0.2 不等于 0.3
## 为什么负数要⽤补码表示
⼗进制数转⼆进制采⽤的是除 2 取余法，⽐如数字 8 转⼆进制的过程如下图：

{% asset_img 30.png %}

「整数类型」的数字在计算机的存储⽅式，就是将⼗进制的数字转换成⼆进制即可。

以`int`类型的数字作为例，`int`类型是 32 位的，其中最⾼位是作为「符号标志位」，正数的符号位是 0 ，负数的符号位是 1 ，剩余的 31 位则表示⼆进制数据。

那么，对于`int`类型的数字 1 的⼆进制数表示如下：

{% asset_img 31.png %}

⽽负数就⽐较特殊了点，负数在计算机中是以补码表示的，所谓的补码就是把正数的⼆进制全部取反再加 1，⽐如 -1 的⼆进制是把数字 1 的⼆进制取反后再加 1，如下图：

{% asset_img 32.png %}

为什么计算机要⽤补码的⽅式来表示负数？在回答这个问题前，我们假设不⽤补码的⽅式来表示负数，⽽只是把最⾼位的符号标志位变为 1 表示负数，如下图过程：

{% asset_img 33.png %}

如果采⽤这种⽅式来表示负数的⼆进制的话，试想⼀下`-2 + 1`的运算过程，如下图：

{% asset_img 34.png %}
{% asset_img 35.png %}

按道理，`-2 + 1 = -1`，但是上⾯的运算过程中得到结果却是 -3，所可以发现，这种负数的表示⽅式是不能⽤常规的加法来计算了，就需要特殊处理，要先判断数字是否为负数，如果是负数就要把加法操作变成减法操作才可以得到正确对结果。

到这⾥，我们就可以回答前⾯提到的「负数为什么要⽤补码⽅式来表示」的问题了。

如果负数不是使⽤补码的⽅式表示，则在做基本对加减法运算的时候，还需要多⼀步操作来判断是否为负数，如果为负数，还得把加法反转成减法，或者把减法反转成加法，这就⾮常不好了，毕竟加减法运算在计算机⾥是很常使⽤的，所以为了性能考虑，应该要尽量简化这个运算过程。

⽽⽤了补码的表示⽅式，对于负数的加减法操作，实际上是和正数加减法操作⼀样的。你可以看到下图，⽤补码表示的负数在运算`-2+1`过程的时候，其结果是正确的：

{% asset_img 36.png %}
{% asset_img 37.png %}

## ⼗进制⼩数与⼆进制的转换
整数⼗进制转⼆进制我们知道了，接下来看看⼩数是怎么转⼆进制的，⼩数部分的转换不同于整数部分，它采⽤的是乘 2 取整法，将⼗进制中的⼩数部分乘以 2 作为⼆进制的⼀位，然后继续取⼩数部分乘以 2 作为下⼀位，直到不存在⼩数为⽌。

以 8.625 转⼆进制作为例：

{% asset_img 38.png %}

最后把「整数部分 + ⼩数部分」结合在⼀起后，其结果就是`1000.101`。

但是，并不是所有⼩数都可以⽤⼆进制表示，前⾯提到的 0.625 ⼩数是⼀个特例，刚好通过乘 2 取整法的⽅式完整的转换成⼆进制。

如果我们⽤相同的⽅式，来把 0.1 转换成⼆进制，过程如下：

{% asset_img 39.png %}

可以发现， 0.1 的⼆进制表示是⽆限循环的。

由于计算机的资源是有限的，所以是没办法⽤⼆进制精确的表示 0.1，只能⽤近似值来表示，就是在有限的精度情况下，最⼤化接近 0.1 的⼆进制数，于是就会造成精度缺失的情况。

对于⼆进制⼩数转⼗进制时，需要注意⼀点，⼩数点后⾯的指数幂是负数。

⽐如，⼆进制`0.1`转成⼗进制就是 2<sup>-1</sup>，也就是⼗进制 0.5 ，⼆进制 0.01 转成⼗进制就是2<sup>-2</sup>，也就是⼗进制 0.25，以此类推。

举个例⼦，⼆进制`1010.101`转⼗进制的过程，如下图：

{% asset_img 40.png %}

## 计算机是怎么存⼩数的
`1000.101`这种⼆进制⼩数是「定点数」形式，代表着⼩数点是定死的，不能移动，如果你移动了它的⼩数点，这个数就变了，就不再是它原来的值了。

然⽽，计算机并不是这样存储⼩数的，计算机存储⼩数采⽤的是浮点数，名字⾥的「浮点」表示⼩数点是可以浮动的。

⽐如`1000.101`这个⼆进制数，可以表示成`1.000101 x 2^3`，类似于数学上的科学记数法。

科学记数法在⼩数点左边只有⼀个数字，⽽且把这种整数部分没有前导 0 的数字称为规格化，⽐如`1.0 x 10^(-9)`是规格化的科学记数法，⽽`0.1 x 10^(-9)`和`10.0 x 10^(-9)`就不是了。

因此，如果⼆进制要⽤到科学记数法，同时要规范化，那么不仅要保证基数为 2，还要保证⼩数点左侧只有 1 位，⽽且必须为 1。

所以通常将`1000.101`这种⼆进制数，规格化表示成`1.000101 x 2^3`，其中，最为关键的是`000101`和 3 这两个东⻄，它就可以包含了这个⼆进制⼩数的所有信息：
* 000101 称为尾数，即⼩数点后⾯的数字；
* 3 称为指数，指定了⼩数点在数据中的位置；

现在绝⼤多数计算机使⽤的浮点数，⼀般采⽤的是 IEEE 制定的国际标准，这种标准形式如下图：

{% asset_img 41.png %}

这三个重要部分的意义如下：
* 符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
* 指数位：指定了⼩数点在数据中的位置，指数可以是负数，也可以是正数，指数位的⻓度越⻓则数值的表达范围就越⼤；
* 尾数位：⼩数点右侧的数字，也就是⼩数部分，⽐如⼆进制`1.0011 x 2^(-2)`，尾数部分就是`0011`，⽽且尾数的⻓度决定了这个数的精度，因此如果要表示精度更⾼的⼩数，则就要提⾼尾数位的⻓度；

⽤ 32 位来表示的浮点数，则称为单精度浮点数，也就是`float`变量，⽽⽤ 64 位来表示的浮点数，称为双精度浮点数，也就是`double`变量，它们的结构如下：

{% asset_img 42.png %}

可以看到：
* `double`的尾数部分是 52 位，`float`的尾数部分是 23 位，由于同时都带有⼀个固定隐含位，所以`double`有 53 个⼆进制有效位，`float`有 24 个⼆进制有效位，所以所以它们的精度在⼗进制中分别是`log10(2^53)`约等于 15.95 和`log10(2^24)`约等于 7.22 位，因此`double`的有效数字是`15~16`位，`float`的有效数字是`7~8`位，这些是有效位是包含整数部分和⼩数部分；
* `double`的指数部分是 11 位，⽽`float`的指数位是 8 位，意味着`double`相⽐`float`能表示更⼤的数值范围；

那⼆进制⼩数，是如何转换成⼆进制浮点数的呢？

以 10.625 作为例⼦，看看这个数字在 float ⾥是如何存储的。

{% asset_img 43.png %}

⾸先，我们计算出 10.625 的⼆进制⼩数为`1010.101`。

然后把⼩数点，移动到第⼀个有效数字后⾯，即将`1010.101`右移 3 位成`1.010101`，右移 3 位就代表 +3，左移 3 位就是 -3。

`float`中的「指数位」就跟这⾥移动的位数有关系，把移动的位数再加上「偏移量」，`float`的话偏移量是 127，相加后就是指数位的值了，即指数位这 8 位存的是 10000010 （⼗进制 130），因此你可以认为「指数位」相当于指明了⼩数点在数据中的位置。

`1.010101`这个数的⼩数点右侧的数字就是`float`⾥的「尾数位」，由于尾数位是 23 位，则后⾯要补充 0，所以最终尾数位存储的数字是`01010100000000000000000`。

在算指数的时候，你可能会有疑问为什么要加上偏移量呢？

指数可能是正数，也可能是负数，即指数是有符号的整数，⽽有符号整数的计算是⽐⽆符号整数麻烦的，所以为了减少不必要的麻烦，在实际存储指数的时候，需要把指数转换成⽆符号整数。

`float`的指数部分是 8 位，IEEE 标准规定单精度浮点的指数取值范围是`-126~+127`，于是为了把指数转换成⽆符号整数，就要加个偏移量，⽐如`float`的指数偏移量是 127，这样指数就不会出现负数了。

⽐如，指数如果是 8，则实际存储的指数是 8 + 127（偏移量）= 135，即把 135 转换为⼆进制之后再存储，⽽当我们需要计算实际的⼗进制数的时候，再把指数减去「偏移量」即可。

移动后的⼩数点左侧的有效位（即 1）消失了，它并没有存储到`float`⾥。

这是因为 IEEE 标准规定，⼆进制浮点数的⼩数点左侧只能有 1 位，并且还只能是 1，既然这⼀位永远都是 1，那就可以不⽤存起来了。

于是就让 23 位尾数只存储⼩数部分，然后在计算时会⾃动把这个 1 加上，这样就可以节约 1 位的空间，尾数就能多存⼀位⼩数，相应的精度就更⾼了⼀点。

那么，对于我们在从`float`的⼆进制浮点数转换成⼗进制时，要考虑到这个隐含的 1，转换公式如下：

{% asset_img 44.png %}

举个例⼦，我们把下图这个`float`的数据转换成⼗进制，过程如下：

{% asset_img 45.png %}

## 0.1 + 0.2 == 0.3 ?
并不是所有⼩数都可以⽤「完整」的⼆进制来表示的，⽐如⼗进制 0.1 在转换成⼆进制⼩数的时候，是⼀串⽆限循环的⼆进制数，计算机是⽆法表达⽆限循环的⼆进制数的，毕竟计算机的资源有限。

因此，计算机只能⽤近似值来表示该⼆进制，那么意味着计算机存放的⼩数可能不是⼀个真实值。现在基本都是⽤ IEEE 754 规范的单精度浮点类型或双精度浮点类型来存储⼩数的，根据精度的不同，近似值也会不同。

那计算机是存储 0.1 是⼀个怎么样的⼆进制浮点数呢？

{% asset_img 46.png %}

可以看到，8 位指数部分是`01111011`，23 位的尾数部分是`10011001100110011001101`，可以看到尾数部分是`0011`是⼀直循环的，只不过尾数是有⻓度限制的，所以只会显示⼀部分，所以是⼀个近似值，精度⼗分有限。

接下来，我们看看 0.2 的`float`浮点数：

{% asset_img 47.png %}

可以看到，8 位指数部分是`01111100`，稍微和 0.1 的指数不同，23 位的尾数部分是`10011001100110011001101`和 0.1 的尾数部分是相同的，也是⼀个近似值。

0.1 的⼆进制浮点数转换成⼗进制的结果是`0.100000001490116119384765625`：

{% asset_img 48.png %}

0.2 的⼆进制浮点数转换成⼗进制的结果是`0.20000000298023223876953125`：

{% asset_img 49.png %}

这两个结果相加就是`0.300000004470348358154296875`：

{% asset_img 50.png %}

所以，你会看到在计算机中`0.1+0.2`并不等于完整的 0.3。

这主要是因为有的⼩数⽆法可以⽤「完整」的⼆进制来表示，所以计算机⾥只能采⽤近似数的⽅式来保存，那两个近似数相加，得到的必然也是⼀个近似数。

我们在 JavaScript ⾥执⾏`0.1+0.2`，你会得到下⾯这个结果：

{% asset_img 51.png %}

结果和我们前⾯推到的类似，因为 JavaScript 对于数字都是使⽤ IEEE 754 标准下的双精度浮点类型来存储的。

⽽我们⼆进制只能精准表达 2 除尽的数字 1/2, 1/4, 1/8，但是对于 0.1(1/10) 和 0.2(1/5)，在⼆进制中都⽆法精准表示时，需要根据精度舍⼊。

我们⼈类熟悉的⼗进制运算系统，可以精准表达 2 和 5 除尽的数字，例如 1/2, 1/4, 1/5(0.2), 1/8,1/10(0.1)。

当然，⼗进制也有⽆法除尽的地⽅，例如 1/3, 1/7，也需要根据精度舍⼊。